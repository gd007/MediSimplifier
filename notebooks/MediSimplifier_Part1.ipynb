{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Project Introduction & Overview\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Project Header\n",
    "\n",
    "# MediSimplifier\n",
    "## Medical Text Simplification Using LoRA Fine-Tuning\n",
    "\n",
    "**Course:** Technion DS25 Deep Learning Final Project  \n",
    "**Students:** Guy Dor, Shmulik Avraham  \n",
    "**Deadline:** February 8, 2026  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Project Motivation & Objectives\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Medical discharge summaries are critical documents that patients receive upon leaving healthcare facilities. However, these documents are typically written in complex medical terminology that is difficult for most patients to understand. Studies show that the average medical document is written at a college reading level, while the average American adult reads at an 8th-grade level. This gap leads to:\n",
    "\n",
    "- Poor medication adherence\n",
    "- Missed follow-up appointments\n",
    "- Increased hospital readmissions\n",
    "- Patient anxiety and confusion\n",
    "\n",
    "### Solution\n",
    "\n",
    "We propose **MediSimplifier**, a system that uses fine-tuned Large Language Models (LLMs) to automatically simplify medical discharge summaries to a 6th-grade reading level while preserving all critical medical information.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. **Simplify** complex medical discharge summaries into plain language accessible to patients with a 6th-grade reading level\n",
    "| Reason | Explanation |\n",
    "|--------|-------------|\n",
    "| **Health literacy statistics** | About 36% of US adults have basic or below-basic health literacy |\n",
    "| **AMA recommendation** | American Medical Association recommends patient materials at 6th grade level |\n",
    "| **Wide accessibility** | Most adults can comfortably read at this level, regardless of education |\n",
    "| **Standard practice** | CDC, NIH, and major hospitals target 6th-8th grade for patient materials |\n",
    "\n",
    "3. **Compare** medical-domain pre-trained LLMs (OpenBioLLM-8B, BioMistral-7B-DARE) against general-purpose LLMs (Mistral-7B)\n",
    "4. **Evaluate** the effectiveness of LoRA fine-tuning compared to zero-shot performance\n",
    "5. **Conduct** comprehensive ablation studies on LoRA hyperparameters (rank, target modules) and dataset size\n",
    "6. **Answer** five research questions about medical text simplification using LLMs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 High-Level Architecture\n",
    "\n",
    "The MediSimplifier project follows a 5-stage pipeline:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         MediSimplifier Pipeline Architecture                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   STAGE 1            ‚îÇ     ‚îÇ   STAGE 2            ‚îÇ     ‚îÇ   STAGE 3            ‚îÇ\n",
    "‚îÇ   Ground Truth       ‚îÇ     ‚îÇ   Baseline           ‚îÇ     ‚îÇ   LoRA               ‚îÇ\n",
    "‚îÇ   Generation         ‚îÇ     ‚îÇ   Evaluation         ‚îÇ     ‚îÇ   Fine-Tuning        ‚îÇ\n",
    "‚îÇ                      ‚îÇ     ‚îÇ                      ‚îÇ     ‚îÇ                      ‚îÇ\n",
    "‚îÇ   Claude Opus 4.5    ‚îÇ     ‚îÇ   Zero-Shot          ‚îÇ     ‚îÇ   Train Student      ‚îÇ\n",
    "‚îÇ   (Teacher Model)    ‚îÇ     ‚îÇ   Inference          ‚îÇ     ‚îÇ   Models             ‚îÇ\n",
    "‚îÇ         ‚îÇ            ‚îÇ     ‚îÇ         ‚îÇ            ‚îÇ     ‚îÇ         ‚îÇ            ‚îÇ\n",
    "‚îÇ         ‚ñº            ‚îÇ     ‚îÇ         ‚ñº            ‚îÇ     ‚îÇ         ‚ñº            ‚îÇ\n",
    "‚îÇ   Simplified         ‚îÇ     ‚îÇ   Baseline           ‚îÇ     ‚îÇ   LoRA Adapters      ‚îÇ\n",
    "‚îÇ   Texts (Labels)     ‚îÇ     ‚îÇ   Predictions        ‚îÇ     ‚îÇ                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "           ‚îÇ                            ‚îÇ                            ‚îÇ\n",
    "           ‚ñº                            ‚ñº                            ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Instruction        ‚îÇ     ‚îÇ   ROUGE-L, SARI      ‚îÇ     ‚îÇ   STAGE 4            ‚îÇ\n",
    "‚îÇ   Tuning Dataset     ‚îÇ     ‚îÇ   BERTScore          ‚îÇ     ‚îÇ   Post Fine-Tuning   ‚îÇ\n",
    "‚îÇ                      ‚îÇ     ‚îÇ   Flesch-Kincaid     ‚îÇ     ‚îÇ   Evaluation         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                                                      ‚îÇ\n",
    "                                                                      ‚ñº\n",
    "                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                             ‚îÇ                    STAGE 5                         ‚îÇ\n",
    "                             ‚îÇ              Comparison & Analysis                 ‚îÇ\n",
    "                             ‚îÇ                                                    ‚îÇ\n",
    "                             ‚îÇ  ‚Ä¢ Baseline vs Fine-tuned Comparison               ‚îÇ\n",
    "                             ‚îÇ  ‚Ä¢ Medical vs General Model Comparison             ‚îÇ\n",
    "                             ‚îÇ  ‚Ä¢ Ablation Studies (Rank, Modules, Dataset Size)  ‚îÇ\n",
    "                             ‚îÇ  ‚Ä¢ Statistical Significance Tests                  ‚îÇ\n",
    "                             ‚îÇ  ‚Ä¢ Research Questions Answered                     ‚îÇ\n",
    "                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Models Overview\n",
    "\n",
    "| Role | Model | Type | Parameters | Base Architecture | Notes |\n",
    "|------|-------|------|------------|-------------------|-------|\n",
    "| **Teacher** | Claude Opus 4.5 | API | N/A | Anthropic | Generates ground truth simplified texts |\n",
    "| **Student 1** | OpenBioLLM-8B | Medical | 8B | Llama-3 | Pre-trained on biomedical literature |\n",
    "| **Student 2** | BioMistral-7B-DARE | Medical | 7B | Mistral | Pre-trained on PubMed abstracts |\n",
    "| **Student 3** | Mistral-7B | General | 7B | Mistral | Control group (no medical pre-training) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Research Questions\n",
    "\n",
    "This project aims to answer the following research questions:\n",
    "\n",
    "| # | Research Question | Comparison | Expected Outcome |\n",
    "|---|-------------------|------------|------------------|\n",
    "| **RQ1** | Does medical domain pre-training improve simplification? | BioMistral-7B-DARE vs. Mistral-7B | Medical > General |\n",
    "| **RQ2** | Which medical model performs best? | OpenBioLLM-8B vs. BioMistral-7B-DARE | TBD (architecture differences) |\n",
    "| **RQ3** | How much does LoRA fine-tuning improve over zero-shot? | Baseline vs. Fine-tuned | Significant improvement |\n",
    "| **RQ4** | What is the optimal LoRA rank? | r = 8, 16, 32 | r=16 optimal |\n",
    "| **RQ5** | Does medical vs. general gap persist after fine-tuning? | All models post-training | Gap narrows |\n",
    "| **RQ6** | Which attention modules matter most for simplification? | q_only vs. q_v vs. all_attn | q_v sufficient |\n",
    "| **RQ7** | How much labeled data is sufficient for LoRA fine-tuning? | 2K vs. 4K vs. 8K samples | Diminishing returns |\n",
    "| **RQ8** | Is there inverse correlation between baseline and improvement? | Zero-shot vs. fine-tuned gains | Worse baseline ‚Üí bigger gain |\n",
    "| **RQ9** | What is the parameter efficiency of target module selection? | Trainable params vs. metric gains | Diminishing returns |\n",
    "| **RQ10** | Which model produces most consistent output quality? | FK-Grade standard deviation | Lower std = more reliable |\n",
    "| **RQ11** | Do fine-tuned models match ground truth readability? | Model FK vs. reference FK | Within ¬±1 grade |\n",
    "\n",
    "**Research Question Details:**\n",
    "\n",
    "- **RQ1-RQ5:** Core research questions addressing domain pre-training effects and LoRA fine-tuning impact on medical text simplification.\n",
    "\n",
    "- **RQ6 (Target Modules):** The LoRA paper (Hu et al. 2021) recommends q_proj and v_proj, but text simplification may benefit from different configurations. This ablation tests minimal adaptation (q_only), standard LoRA (q_v), and full attention adaptation (all_attn).\n",
    "\n",
    "- **RQ7 (Dataset Size):** With expensive API labeling (~$150-200 for ground truth generation), understanding data efficiency is crucial. This ablation tests whether 25% (2K), 50% (4K), or 100% (8K) of labeled data is sufficient for effective fine-tuning.\n",
    "\n",
    "- **RQ8 (Baseline-Improvement Correlation):** Investigates whether models with weaker zero-shot performance benefit more from fine-tuning, which would suggest LoRA is particularly effective at addressing initial capability gaps.\n",
    "\n",
    "- **RQ9 (Parameter Efficiency):** Analyzes the trade-off between additional trainable parameters (8.4M ‚Üí 27.3M) and metric improvements to determine if full attention adaptation is cost-effective.\n",
    "\n",
    "- **RQ10 (Output Consistency):** Beyond average quality, examines output variance to identify which model produces the most reliable simplifications across diverse medical texts.\n",
    "\n",
    "- **RQ11 (Ground Truth Ceiling):** Tests whether fine-tuned models can match the readability level of Claude-generated ground truth, validating the effectiveness of knowledge distillation.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Notebook Structure\n",
    "\n",
    "This project is organized into **10 chapters**, with a hardware transition at Chapter 6:\n",
    "\n",
    "| Chapter | Title | Hardware | Description |\n",
    "|---------|-------|----------|-------------|\n",
    "| 1 | Project Introduction & Overview | M4 Max | Motivation, objectives, architecture, research questions |\n",
    "| 2 | Environment Setup | M4 Max | Dependencies, hardware verification, configuration |\n",
    "| 3 | Dataset Preparation | M4 Max | Data loading, filtering, statistics |\n",
    "| 4 | Ground Truth Generation | M4 Max | Claude API labeling, quality check, split, instruction dataset |\n",
    "| 5 | Baseline Evaluation (Zero-Shot) | H200 SXM | Zero-shot inference with metrics against ground truth |\n",
    "| 6 | LoRA Fine-Tuning | H200 SXM | Ablation studies (18 runs) + full training (3 runs) |\n",
    "| 7 | Post Fine-Tuning Evaluation | H200 SXM | Evaluation and comparison with baseline |\n",
    "| 8 | Results Analysis | M4 Max | Comprehensive analysis and research question answers |\n",
    "| 9 | Conclusions | M4 Max | Summary, limitations, future work |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Hardware Configuration\n",
    "\n",
    "This project uses a **dual-hardware approach** to balance local development with cloud GPU training:\n",
    "\n",
    "| Phase | Hardware | Backend | Chapters | Purpose |\n",
    "|-------|----------|---------|----------|---------|\n",
    "| **Development & Baseline** | Apple M4 Max, 128GB RAM | PyTorch MPS | 1-4, 8+ | Data prep, API labeling, zero-shot evaluation |\n",
    "| **Training & Ablation** | RunPod H200 SXM, 80GB HBM3 | PyTorch CUDA | 5-7 | LoRA fine-tuning, ablation studies |\n",
    "\n",
    "### Local Hardware \n",
    "\n",
    "| Component | Specification |\n",
    "|-----------|---------------|\n",
    "| **Processor** | Apple M4 Max |\n",
    "| **Memory** | 128GB Unified RAM |\n",
    "| **GPU Backend** | PyTorch MPS (Metal Performance Shaders) |\n",
    "| **Storage** | SSD (sufficient for models and data) |\n",
    "| **Use Cases** | Dataset preparation, Claude API calls, zero-shot inference |\n",
    "\n",
    "### Cloud Hardware \n",
    "\n",
    "| Component | Specification |\n",
    "|-----------|---------------|\n",
    "| **Provider** | RunPod |\n",
    "| **GPU** | NVIDIA H200 SXM |\n",
    "| **GPU Memory** | 80GB HBM3 |\n",
    "| **Backend** | PyTorch CUDA |\n",
    "| **Use Cases** | LoRA fine-tuning, ablation studies, post-training evaluation |\n",
    "\n",
    "### Hardware Migration Rationale\n",
    "\n",
    "The project initially attempted Chapter 6 on M4 Max (MPS backend), but encountered significant performance issues:\n",
    "- **MPS Training Speed:** ~8 hours per ablation run (vs. ~20 min expected on CUDA)\n",
    "- **Memory Pressure:** 80GB+ swap usage during training\n",
    "- **Resolution:** Migrated to RunPod H200 SXM for ~10x speedup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Project Deliverables\n",
    "\n",
    "| Deliverable | Format | Description |\n",
    "|-------------|--------|-------------|\n",
    "| **Technical Report** | PDF (IEEE format) | Full methodology, results, and analysis |\n",
    "| **Presentation** | PPTX/PDF (15 slides) | Project overview, methodology, results, conclusions |\n",
    "| **Code Repository** | GitHub | All source code, configs, and notebooks |\n",
    "| **Trained Models** | HuggingFace Hub | LoRA adapters for each student model |\n",
    "| **Dataset** | HuggingFace Hub | Labeled instruction tuning dataset |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Environment Setup\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Install Dependencies\n",
    "\n",
    "Install all required Python packages for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DEPENDENCY VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "Python Version: 3.11.13\n",
      "\n",
      "‚úì torch                (v2.9.1          ) - PyTorch deep learning framework\n",
      "‚úì transformers         (v4.57.3         ) - HuggingFace library for pre-trained language models\n",
      "‚úì peft                 (v0.18.1         ) - Parameter-Efficient Fine-Tuning library for LoRA\n",
      "‚úì datasets             (v4.4.2          ) - HuggingFace library for dataset loading and processing\n",
      "‚úì evaluate             (v0.4.6          ) - HuggingFace library for evaluation metrics\n",
      "‚úì rouge-score          (vunknown        ) - ROUGE metrics for text summarization evaluation\n",
      "‚úì bert-score           (v0.3.12         ) - BERTScore for semantic similarity evaluation\n",
      "‚úì sacrebleu            (v2.5.1          ) - BLEU score implementation\n",
      "‚úì textstat             (v(0, 7, 12)     ) - Readability metrics (Flesch-Kincaid, etc.)\n",
      "‚úì easse                (vunknown        ) - SARI metric for sentence simplification evaluation\n",
      "‚úì accelerate           (v1.12.0         ) - HuggingFace library for distributed training\n",
      "‚úì bitsandbytes         (v0.49.1         ) - Library for 8-bit quantization (memory efficiency)\n",
      "‚úì pandas               (v2.3.3          ) - Data manipulation and analysis\n",
      "‚úì numpy                (v2.4.1          ) - Numerical computing\n",
      "‚úì matplotlib           (v3.10.8         ) - Plotting library\n",
      "‚úì seaborn              (v0.13.2         ) - Statistical data visualization\n",
      "‚úì anthropic            (v0.75.0         ) - Official Anthropic API client for Claude\n",
      "‚úì tqdm                 (v4.67.1         ) - Progress bars for loops\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä Results:\n",
      "   Installed: 18/18 packages\n",
      "   Missing:   0/18 packages\n",
      "\n",
      "‚úì All required dependencies are installed!\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Verify Required Libraries Are Installed\n",
    "# ============================================================================\n",
    "\n",
    "import importlib  # Import the importlib module to check if packages are installed\n",
    "import subprocess  # Import subprocess to get package versions\n",
    "import sys  # Import sys for Python version info\n",
    "\n",
    "# Define all required packages with their import names and descriptions\n",
    "# Format: (import_name, pip_name, description)\n",
    "REQUIRED_PACKAGES = [  # List of tuples containing package information\n",
    "    # Core deep learning and transformer libraries\n",
    "    (\"torch\", \"torch\", \"PyTorch deep learning framework\"),  # PyTorch\n",
    "    (\"transformers\", \"transformers\", \"HuggingFace library for pre-trained language models\"),  # Transformers\n",
    "    (\"peft\", \"peft\", \"Parameter-Efficient Fine-Tuning library for LoRA\"),  # PEFT\n",
    "    (\"datasets\", \"datasets\", \"HuggingFace library for dataset loading and processing\"),  # Datasets\n",
    "    (\"evaluate\", \"evaluate\", \"HuggingFace library for evaluation metrics\"),  # Evaluate\n",
    "    \n",
    "    # Evaluation metric libraries\n",
    "    (\"rouge_score\", \"rouge-score\", \"ROUGE metrics for text summarization evaluation\"),  # ROUGE\n",
    "    (\"bert_score\", \"bert-score\", \"BERTScore for semantic similarity evaluation\"),  # BERTScore\n",
    "    (\"sacrebleu\", \"sacrebleu\", \"BLEU score implementation\"),  # SacreBLEU\n",
    "    (\"textstat\", \"textstat\", \"Readability metrics (Flesch-Kincaid, etc.)\"),  # Textstat\n",
    "    (\"easse\", \"easse\", \"SARI metric for sentence simplification evaluation\"),  # EASSE\n",
    "    \n",
    "    # Training acceleration libraries\n",
    "    (\"accelerate\", \"accelerate\", \"HuggingFace library for distributed training\"),  # Accelerate\n",
    "    (\"bitsandbytes\", \"bitsandbytes\", \"Library for 8-bit quantization (memory efficiency)\"),  # BitsAndBytes\n",
    "    \n",
    "    # Data analysis and visualization libraries\n",
    "    (\"pandas\", \"pandas\", \"Data manipulation and analysis\"),  # Pandas\n",
    "    (\"numpy\", \"numpy\", \"Numerical computing\"),  # NumPy\n",
    "    (\"matplotlib\", \"matplotlib\", \"Plotting library\"),  # Matplotlib\n",
    "    (\"seaborn\", \"seaborn\", \"Statistical data visualization\"),  # Seaborn\n",
    "    \n",
    "    # API and utility libraries\n",
    "    (\"anthropic\", \"anthropic\", \"Official Anthropic API client for Claude\"),  # Anthropic\n",
    "    (\"tqdm\", \"tqdm\", \"Progress bars for loops\"),  # TQDM\n",
    "]\n",
    "\n",
    "def check_package_installed(import_name):  # Function to check if a package is installed\n",
    "    \"\"\"Check if a package is installed and return its version if available.\"\"\"\n",
    "    try:  # Try to import the package\n",
    "        module = importlib.import_module(import_name)  # Attempt to import\n",
    "        version = getattr(module, '__version__', 'unknown')  # Get version if available\n",
    "        return True, version  # Return success and version\n",
    "    except ImportError:  # Package not installed\n",
    "        return False, None  # Return failure\n",
    "\n",
    "# ============================================================================\n",
    "# Check All Required Packages\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)  # Print separator line\n",
    "print(\"DEPENDENCY VERIFICATION\")  # Print section header\n",
    "print(\"=\" * 70)  # Print separator line\n",
    "print(f\"\\nPython Version: {sys.version.split()[0]}\\n\")  # Display Python version\n",
    "\n",
    "installed_packages = []  # List to store installed packages\n",
    "missing_packages = []  # List to store missing packages\n",
    "\n",
    "# Check each required package\n",
    "for import_name, pip_name, description in REQUIRED_PACKAGES:  # Iterate through packages\n",
    "    is_installed, version = check_package_installed(import_name)  # Check if installed\n",
    "    \n",
    "    if is_installed:  # Package is installed\n",
    "        version_str = str(version) if version else 'unknown'  # Convert version to string (handles tuples)\n",
    "        installed_packages.append((import_name, pip_name, version_str, description))  # Add to installed list\n",
    "        print(f\"‚úì {pip_name:20} (v{version_str:15}) - {description}\")  # Print success\n",
    "    else:  # Package is missing\n",
    "        missing_packages.append((import_name, pip_name, description))  # Add to missing list\n",
    "        print(f\"‚úó {pip_name:20} {'MISSING':17} - {description}\")  # Print missing\n",
    "\n",
    "# ============================================================================\n",
    "# Summary and Installation Instructions\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)  # Print separator line\n",
    "print(\"VERIFICATION SUMMARY\")  # Print section header\n",
    "print(\"=\" * 70)  # Print separator line\n",
    "\n",
    "print(f\"\\nüìä Results:\")  # Section header\n",
    "print(f\"   Installed: {len(installed_packages)}/{len(REQUIRED_PACKAGES)} packages\")  # Count installed\n",
    "print(f\"   Missing:   {len(missing_packages)}/{len(REQUIRED_PACKAGES)} packages\")  # Count missing\n",
    "\n",
    "if missing_packages:  # If there are missing packages\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_packages)} required package(s) are missing!\\n\")  # Warning\n",
    "    print(\"To install missing packages, run the following command(s):\\n\")  # Instructions\n",
    "    \n",
    "    # Generate pip install command for all missing packages\n",
    "    missing_pip_names = [pip_name for _, pip_name, _ in missing_packages]  # Get pip names\n",
    "    install_command = \"pip install \" + \" \".join(missing_pip_names)  # Create install command\n",
    "    print(f\"   {install_command}\\n\")  # Print install command\n",
    "    \n",
    "    print(\"Or install individually:\")  # Alternative instructions\n",
    "    for import_name, pip_name, description in missing_packages:  # Iterate through missing\n",
    "        print(f\"   pip install {pip_name}  # {description}\")  # Print individual command\n",
    "    \n",
    "    # Set flag indicating not all dependencies are met\n",
    "    ALL_DEPENDENCIES_MET = False  # Set flag to False\n",
    "    print(f\"\\n‚ùå Please install missing packages before proceeding.\")  # Final warning\n",
    "else:  # All packages installed\n",
    "    ALL_DEPENDENCIES_MET = True  # Set flag to True\n",
    "    print(f\"\\n‚úì All required dependencies are installed!\")  # Success message\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)  # Print separator line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import Libraries\n",
    "\n",
    "Import all required libraries and set random seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All libraries imported successfully\n",
      "‚úì Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Standard Library Imports\n",
    "# ============================================================================\n",
    "\n",
    "import os  # Operating system interface for environment variables and file operations\n",
    "import sys  # System-specific parameters and functions\n",
    "import json  # JSON encoding and decoding for saving/loading configurations\n",
    "import random  # Random number generation for reproducibility\n",
    "import warnings  # Warning control to suppress unnecessary messages\n",
    "from pathlib import Path  # Object-oriented filesystem paths\n",
    "from datetime import datetime  # Date and time operations for logging\n",
    "from typing import Dict, List, Optional, Tuple, Any  # Type hints for better code documentation\n",
    "\n",
    "# ============================================================================\n",
    "# Data Science Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np  # Numerical computing library for array operations\n",
    "import pandas as pd  # Data manipulation and analysis library\n",
    "\n",
    "# ============================================================================\n",
    "# Visualization Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt  # Plotting library for creating figures\n",
    "import seaborn as sns  # Statistical visualization library built on matplotlib\n",
    "\n",
    "# ============================================================================\n",
    "# PyTorch and Deep Learning Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import torch  # PyTorch deep learning framework\n",
    "import torch.nn as nn  # Neural network modules\n",
    "from torch.utils.data import DataLoader  # Data loading utilities\n",
    "\n",
    "# ============================================================================\n",
    "# HuggingFace Libraries\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import (  # HuggingFace transformers library\n",
    "    AutoTokenizer,  # Automatic tokenizer loading based on model name\n",
    "    AutoModelForCausalLM,  # Automatic causal language model loading\n",
    "    TrainingArguments,  # Configuration for training parameters\n",
    "    Trainer,  # High-level training API\n",
    "    DataCollatorForLanguageModeling,  # Data collator for language modeling\n",
    "    BitsAndBytesConfig,  # Configuration for quantization\n",
    ")\n",
    "\n",
    "from peft import (  # Parameter-Efficient Fine-Tuning library\n",
    "    LoraConfig,  # LoRA configuration\n",
    "    get_peft_model,  # Function to wrap model with LoRA\n",
    "    PeftModel,  # PEFT model class\n",
    "    TaskType,  # Task type enum for LoRA\n",
    ")\n",
    "\n",
    "from datasets import (  # HuggingFace datasets library\n",
    "    load_dataset,  # Function to load datasets from HuggingFace Hub\n",
    "    Dataset,  # Dataset class\n",
    "    DatasetDict,  # Dictionary of datasets for train/val/test splits\n",
    ")\n",
    "\n",
    "import evaluate  # HuggingFace evaluation metrics library\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation Metric Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import textstat  # Library for calculating readability metrics\n",
    "from rouge_score import rouge_scorer  # ROUGE metric implementation\n",
    "\n",
    "# ============================================================================\n",
    "# API and Utility Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import anthropic  # Official Anthropic API client for Claude\n",
    "from tqdm.auto import tqdm  # Progress bar library with auto notebook detection\n",
    "\n",
    "# ============================================================================\n",
    "# Suppress Warnings\n",
    "# ============================================================================\n",
    "\n",
    "warnings.filterwarnings('ignore')  # Suppress all warnings for cleaner output\n",
    "\n",
    "# ============================================================================\n",
    "# Set Random Seeds for Reproducibility\n",
    "# ============================================================================\n",
    "\n",
    "SEED = 42  # Define global random seed for reproducibility\n",
    "\n",
    "random.seed(SEED)  # Set Python random seed\n",
    "np.random.seed(SEED)  # Set NumPy random seed\n",
    "torch.manual_seed(SEED)  # Set PyTorch CPU random seed\n",
    "\n",
    "# Set PyTorch MPS (Metal Performance Shaders) seed if available\n",
    "if torch.backends.mps.is_available():  # Check if MPS backend is available\n",
    "    torch.mps.manual_seed(SEED)  # Set MPS random seed for Apple Silicon\n",
    "\n",
    "# Set PyTorch CUDA seed if available (for potential GPU usage)\n",
    "if torch.cuda.is_available():  # Check if CUDA is available\n",
    "    torch.cuda.manual_seed_all(SEED)  # Set CUDA random seed for all GPUs\n",
    "\n",
    "# Print confirmation of imports and seed setting\n",
    "print(f\"‚úì All libraries imported successfully\")  # Confirmation message\n",
    "print(f\"‚úì Random seed set to: {SEED}\")  # Display seed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Verify Hardware Backend\n",
    "\n",
    "Check and configure the appropriate hardware backend (MPS for Apple Silicon, CUDA for NVIDIA, or CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "üì± Device: Apple Silicon (MPS)\n",
      "üîß PyTorch Device: mps\n",
      "üêç PyTorch Version: 2.9.1\n",
      "üñ•Ô∏è  Python Version: 3.11.13\n",
      "\n",
      "üçé Apple Silicon MPS Backend Active\n",
      "   - MPS Available: True\n",
      "   - MPS Built: True\n",
      "\n",
      "============================================================\n",
      "‚úì Hardware backend configured: mps\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Hardware Backend Detection and Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Check for MPS (Metal Performance Shaders) availability on Apple Silicon\n",
    "if torch.backends.mps.is_available():  # Check if MPS backend is available\n",
    "    # Verify that MPS is actually built into this PyTorch installation\n",
    "    if torch.backends.mps.is_built():  # Double-check MPS is built\n",
    "        DEVICE = torch.device(\"mps\")  # Set device to MPS for Apple Silicon\n",
    "        DEVICE_NAME = \"Apple Silicon (MPS)\"  # Human-readable device name\n",
    "    else:  # MPS available but not built\n",
    "        DEVICE = torch.device(\"cpu\")  # Fallback to CPU\n",
    "        DEVICE_NAME = \"CPU (MPS not built)\"  # Indicate MPS not built\n",
    "# Check for CUDA availability on NVIDIA GPUs\n",
    "elif torch.cuda.is_available():  # Check if CUDA is available\n",
    "    DEVICE = torch.device(\"cuda\")  # Set device to CUDA for NVIDIA GPUs\n",
    "    DEVICE_NAME = f\"NVIDIA GPU ({torch.cuda.get_device_name(0)})\"  # Get GPU name\n",
    "# Fallback to CPU if no GPU acceleration available\n",
    "else:  # No GPU acceleration available\n",
    "    DEVICE = torch.device(\"cpu\")  # Set device to CPU\n",
    "    DEVICE_NAME = \"CPU\"  # Human-readable device name\n",
    "\n",
    "# ============================================================================\n",
    "# Print Hardware Information\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"HARDWARE CONFIGURATION\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "print(f\"\\nüì± Device: {DEVICE_NAME}\")  # Display selected device\n",
    "print(f\"üîß PyTorch Device: {DEVICE}\")  # Display PyTorch device object\n",
    "print(f\"üêç PyTorch Version: {torch.__version__}\")  # Display PyTorch version\n",
    "print(f\"üñ•Ô∏è  Python Version: {sys.version.split()[0]}\")  # Display Python version\n",
    "\n",
    "# Display MPS-specific information if on Apple Silicon\n",
    "if DEVICE.type == \"mps\":  # Check if using MPS backend\n",
    "    print(f\"\\nüçé Apple Silicon MPS Backend Active\")  # Confirm MPS is active\n",
    "    print(f\"   - MPS Available: {torch.backends.mps.is_available()}\")  # MPS availability\n",
    "    print(f\"   - MPS Built: {torch.backends.mps.is_built()}\")  # MPS built status\n",
    "\n",
    "# Display CUDA-specific information if on NVIDIA GPU\n",
    "elif DEVICE.type == \"cuda\":  # Check if using CUDA backend\n",
    "    print(f\"\\nüéÆ NVIDIA CUDA Backend Active\")  # Confirm CUDA is active\n",
    "    print(f\"   - CUDA Version: {torch.version.cuda}\")  # CUDA version\n",
    "    print(f\"   - GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")  # GPU memory\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line\n",
    "print(f\"‚úì Hardware backend configured: {DEVICE}\")  # Confirmation message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Setup API Keys\n",
    "\n",
    "Verify that the Anthropic API key is configured for Claude Opus 4.5 access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ANTHROPIC_API_KEY found: sk-a********************DgAA\n",
      "\n",
      "üìä API Key Status: Configured\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Anthropic API Key Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Check if ANTHROPIC_API_KEY environment variable is set\n",
    "ANTHROPIC_API_KEY = os.environ.get(\"ANTHROPIC_API_KEY\")  # Retrieve API key from environment\n",
    "\n",
    "# Verify API key is available\n",
    "if ANTHROPIC_API_KEY:  # If API key is set\n",
    "    # Mask the API key for security (show only first and last 4 characters)\n",
    "    masked_key = ANTHROPIC_API_KEY[:4] + \"*\" * 20 + ANTHROPIC_API_KEY[-4:]  # Create masked version\n",
    "    print(f\"‚úì ANTHROPIC_API_KEY found: {masked_key}\")  # Display masked key\n",
    "    API_KEY_CONFIGURED = True  # Set flag indicating API key is configured\n",
    "else:  # If API key is not set\n",
    "    print(\"‚ö†Ô∏è  WARNING: ANTHROPIC_API_KEY environment variable is not set!\")  # Warning message\n",
    "    print(\"   To set the API key, run one of the following:\")  # Instructions\n",
    "    print(\"   - macOS/Linux: export ANTHROPIC_API_KEY='your-api-key'\")  # macOS/Linux command\n",
    "    print(\"   - Or set it in this notebook: os.environ['ANTHROPIC_API_KEY'] = 'your-api-key'\")  # Notebook option\n",
    "    API_KEY_CONFIGURED = False  # Set flag indicating API key is not configured\n",
    "\n",
    "# Store the API key configuration status for later use\n",
    "print(f\"\\nüìä API Key Status: {'Configured' if API_KEY_CONFIGURED else 'Not Configured'}\")  # Display status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Create Directory Structure\n",
    "\n",
    "Create the project directory structure for organizing outputs, models, and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created directory: medisimplifier\n",
      "‚úì Created directory: medisimplifier/data\n",
      "‚úì Created directory: medisimplifier/models\n",
      "‚úì Created directory: medisimplifier/results\n",
      "‚úì Created directory: medisimplifier/checkpoints\n",
      "‚úì Created directory: medisimplifier/logs\n",
      "‚úì Created directory: medisimplifier/results/figures\n",
      "‚úì Created directory: medisimplifier/results/tables\n",
      "‚úì Created directory: medisimplifier/HFDatasets\n",
      "‚úì Created directory: medisimplifier/HFBaseModels\n",
      "\n",
      "============================================================\n",
      "PROJECT DIRECTORY STRUCTURE\n",
      "============================================================\n",
      "\n",
      "medisimplifier/\n",
      "‚îú‚îÄ‚îÄ data/              # Processed datasets and instruction tuning data\n",
      "‚îú‚îÄ‚îÄ models/            # Trained LoRA adapters (project output)\n",
      "‚îú‚îÄ‚îÄ results/           # Evaluation results and metrics\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ figures/       # Generated plots and visualizations\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ tables/        # Exported tables (CSV, LaTeX)\n",
      "‚îú‚îÄ‚îÄ checkpoints/       # Training checkpoints\n",
      "‚îú‚îÄ‚îÄ logs/              # Training and API logs\n",
      "‚îú‚îÄ‚îÄ HFDatasets/        # HuggingFace datasets cache (raw data)\n",
      "‚îî‚îÄ‚îÄ HFBaseModels/      # HuggingFace base models cache (~60GB)\n",
      "\n",
      "============================================================\n",
      "‚úì All directories created successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Project Directory Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Define the root directory for all project files\n",
    "PROJECT_ROOT = Path(\"./medisimplifier\")  # Root directory for the project\n",
    "\n",
    "# Define subdirectories for organizing different types of files\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"  # Directory for datasets and processed data\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"  # Directory for saved models and LoRA adapters\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"  # Directory for evaluation results and metrics\n",
    "CHECKPOINTS_DIR = PROJECT_ROOT / \"checkpoints\"  # Directory for training checkpoints\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"  # Directory for training logs and API logs\n",
    "FIGURES_DIR = RESULTS_DIR / \"figures\"  # Directory for generated figures and plots\n",
    "TABLES_DIR = RESULTS_DIR / \"tables\"  # Directory for exported tables (CSV, LaTeX)\n",
    "\n",
    "# HuggingFace cache directories (to keep all data within project)\n",
    "HF_DATASETS_DIR = PROJECT_ROOT / \"HFDatasets\"  # Directory for HuggingFace datasets cache\n",
    "HF_BASE_MODELS_DIR = PROJECT_ROOT / \"HFBaseModels\"  # Directory for HuggingFace base models cache\n",
    "\n",
    "# Create a list of all directories to create\n",
    "ALL_DIRS = [  # List of all project directories\n",
    "    PROJECT_ROOT,  # Root project directory\n",
    "    DATA_DIR,  # Data directory\n",
    "    MODELS_DIR,  # Models directory\n",
    "    RESULTS_DIR,  # Results directory\n",
    "    CHECKPOINTS_DIR,  # Checkpoints directory\n",
    "    LOGS_DIR,  # Logs directory\n",
    "    FIGURES_DIR,  # Figures directory\n",
    "    TABLES_DIR,  # Tables directory\n",
    "    HF_DATASETS_DIR,  # HuggingFace datasets cache\n",
    "    HF_BASE_MODELS_DIR,  # HuggingFace base models cache\n",
    "]\n",
    "\n",
    "# Create all directories (including parent directories if needed)\n",
    "for directory in ALL_DIRS:  # Iterate through all directories\n",
    "    directory.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    print(f\"‚úì Created directory: {directory}\")  # Print confirmation\n",
    "\n",
    "# ============================================================================\n",
    "# Display Directory Structure\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line\n",
    "print(\"PROJECT DIRECTORY STRUCTURE\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(f\"\"\"\n",
    "{PROJECT_ROOT}/\n",
    "‚îú‚îÄ‚îÄ data/              # Processed datasets and instruction tuning data\n",
    "‚îú‚îÄ‚îÄ models/            # Trained LoRA adapters (project output)\n",
    "‚îú‚îÄ‚îÄ results/           # Evaluation results and metrics\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ figures/       # Generated plots and visualizations\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ tables/        # Exported tables (CSV, LaTeX)\n",
    "‚îú‚îÄ‚îÄ checkpoints/       # Training checkpoints\n",
    "‚îú‚îÄ‚îÄ logs/              # Training and API logs\n",
    "‚îú‚îÄ‚îÄ HFDatasets/        # HuggingFace datasets cache (raw data)\n",
    "‚îî‚îÄ‚îÄ HFBaseModels/      # HuggingFace base models cache (~60GB)\n",
    "\"\"\")  # Display directory tree\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(f\"‚úì All directories created successfully\")  # Confirmation message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Test Anthropic API Connection\n",
    "\n",
    "Initialize the Anthropic client and verify the connection to Claude Opus 4.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANTHROPIC API CONNECTION TEST\n",
      "============================================================\n",
      "\n",
      "‚úì API Connection Successful!\n",
      "  Model: claude-opus-4-5-20251101\n",
      "  Response: Connection successful\n",
      "  Input Tokens: 16\n",
      "  Output Tokens: 5\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Anthropic API Connection Test\n",
    "# ============================================================================\n",
    "\n",
    "# Only proceed if API key is configured\n",
    "if API_KEY_CONFIGURED:  # Check if API key was found\n",
    "    try:  # Attempt to connect to API\n",
    "        # Initialize the Anthropic client with the API key\n",
    "        client = anthropic.Anthropic()  # Create client (uses ANTHROPIC_API_KEY env var)\n",
    "        \n",
    "        # Send a simple test message to verify connection\n",
    "        test_response = client.messages.create(  # Create a test message\n",
    "            model=\"claude-opus-4-5-20251101\",  # Specify Claude Opus 4.5 model\n",
    "            max_tokens=50,  # Limit response tokens for test\n",
    "            messages=[  # Message list\n",
    "                {  # User message\n",
    "                    \"role\": \"user\",  # Role is user\n",
    "                    \"content\": \"Reply with only: 'Connection successful'\"  # Simple test prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response text\n",
    "        response_text = test_response.content[0].text  # Get text from response\n",
    "        \n",
    "        # Display successful connection\n",
    "        print(\"=\" * 60)  # Print separator line\n",
    "        print(\"ANTHROPIC API CONNECTION TEST\")  # Print section header\n",
    "        print(\"=\" * 60)  # Print separator line\n",
    "        print(f\"\\n‚úì API Connection Successful!\")  # Success message\n",
    "        print(f\"  Model: claude-opus-4-5-20251101\")  # Display model used\n",
    "        print(f\"  Response: {response_text}\")  # Display API response\n",
    "        print(f\"  Input Tokens: {test_response.usage.input_tokens}\")  # Display input token count\n",
    "        print(f\"  Output Tokens: {test_response.usage.output_tokens}\")  # Display output token count\n",
    "        \n",
    "        # Store client for later use\n",
    "        API_CLIENT = client  # Store the initialized client\n",
    "        API_CONNECTION_VERIFIED = True  # Set flag for successful connection\n",
    "        \n",
    "    except anthropic.APIError as e:  # Catch API-specific errors\n",
    "        print(f\"‚ùå API Error: {e}\")  # Display API error\n",
    "        API_CLIENT = None  # Set client to None\n",
    "        API_CONNECTION_VERIFIED = False  # Set flag for failed connection\n",
    "        \n",
    "    except Exception as e:  # Catch any other errors\n",
    "        print(f\"‚ùå Error connecting to API: {e}\")  # Display error\n",
    "        API_CLIENT = None  # Set client to None\n",
    "        API_CONNECTION_VERIFIED = False  # Set flag for failed connection\n",
    "\n",
    "else:  # API key not configured\n",
    "    print(\"‚ö†Ô∏è  Skipping API connection test - API key not configured\")  # Warning message\n",
    "    API_CLIENT = None  # Set client to None\n",
    "    API_CONNECTION_VERIFIED = False  # Set flag for no connection\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Define Configuration Constants\n",
    "\n",
    "Define all configuration constants for models, datasets, training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONFIGURATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "üìö Model Configuration:\n",
      "   Teacher Model: claude-opus-4-5-20251101\n",
      "   Student Models: ['OpenBioLLM-8B', 'BioMistral-7B', 'Mistral-7B']\n",
      "\n",
      "üìä Dataset Configuration:\n",
      "   Dataset: starmpcc/Asclepius-Synthetic-Clinical-Notes\n",
      "   Total Samples: 10,000\n",
      "   Split Ratios: Train=0.8, Val=0.1, Test=0.1\n",
      "\n",
      "üîß LoRA Configuration:\n",
      "   Rank (r): 16\n",
      "   Alpha: 32\n",
      "   Dropout: 0.05\n",
      "   Target Modules: ['q_proj', 'v_proj']\n",
      "\n",
      "üèãÔ∏è Training Configuration:\n",
      "   Epochs: 3\n",
      "   Batch Size: 4 (Effective: 16)\n",
      "   Learning Rate: 0.0002\n",
      "   Max Sequence Length: 2048\n",
      "\n",
      "üìè Evaluation Configuration:\n",
      "   Max New Tokens: 512\n",
      "   Target Reading Level: Grade 6\n",
      "\n",
      "============================================================\n",
      "‚úì All configurations defined successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Model Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Teacher model for generating ground truth simplified texts\n",
    "TEACHER_MODEL = \"claude-opus-4-5-20251101\"  # Claude Opus 4.5 model identifier\n",
    "\n",
    "# Student models to fine-tune and compare\n",
    "STUDENT_MODELS = {  # Dictionary of student model names and HuggingFace paths\n",
    "    \"OpenBioLLM-8B\": \"aaditya/Llama3-OpenBioLLM-8B\",  # Medical LLM based on Llama-3\n",
    "    \"BioMistral-7B-DARE\": \"BioMistral/BioMistral-7B-DARE\",  # Medical LLM trained on PubMed\n",
    "    \"Mistral-7B\": \"mistralai/Mistral-7B-Instruct-v0.2\",  # General-purpose LLM (control)\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Dataset source and sampling configuration\n",
    "DATASET_NAME = \"starmpcc/Asclepius-Synthetic-Clinical-Notes\"  # HuggingFace dataset path\n",
    "TOTAL_SAMPLES = 10000  # Total number of samples to use from the dataset\n",
    "MIN_NOTE_LENGTH = 200  # Minimum character length for clinical notes (filter short notes)\n",
    "\n",
    "# Train/Validation/Test split ratios\n",
    "TRAIN_RATIO = 0.8  # 80% of data for training\n",
    "VAL_RATIO = 0.1  # 10% of data for validation\n",
    "TEST_RATIO = 0.1  # 10% of data for testing\n",
    "\n",
    "# ============================================================================\n",
    "# LoRA Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# LoRA hyperparameters for fine-tuning\n",
    "LORA_R = 16  # LoRA rank (dimensionality of low-rank matrices)\n",
    "LORA_ALPHA = 32  # LoRA alpha (scaling factor, typically 2x rank)\n",
    "LORA_DROPOUT = 0.05  # Dropout probability for LoRA layers\n",
    "LORA_TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # Target attention modules for LoRA\n",
    "\n",
    "# ============================================================================\n",
    "# Training Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 3  # Number of training epochs\n",
    "BATCH_SIZE = 4  # Training batch size (adjust based on memory)\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "LEARNING_RATE = 2e-4  # Learning rate for AdamW optimizer\n",
    "WEIGHT_DECAY = 0.01  # Weight decay for regularization\n",
    "WARMUP_RATIO = 0.03  # Proportion of training for learning rate warmup\n",
    "MAX_SEQ_LENGTH = 2048  # Maximum sequence length for tokenization\n",
    "\n",
    "# ============================================================================\n",
    "# Evaluation Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Inference and evaluation parameters\n",
    "MAX_NEW_TOKENS = 512  # Maximum tokens to generate during inference\n",
    "TARGET_READING_LEVEL = 6  # Target Flesch-Kincaid grade level (6th grade)\n",
    "BASELINE_EVAL_SAMPLES = 100  # Number of samples for baseline evaluation\n",
    "\n",
    "# ============================================================================\n",
    "# API Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# Anthropic API parameters\n",
    "API_MAX_TOKENS = 1024  # Maximum tokens for Claude API responses\n",
    "API_RATE_LIMIT_DELAY = 1.0  # Delay between API calls (seconds) to avoid rate limiting\n",
    "CHECKPOINT_INTERVAL = 100  # Save checkpoint every N samples during ground truth generation\n",
    "\n",
    "# ============================================================================\n",
    "# Print Configuration Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"CONFIGURATION SUMMARY\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "print(\"\\nüìö Model Configuration:\")  # Model section\n",
    "print(f\"   Teacher Model: {TEACHER_MODEL}\")  # Display teacher model\n",
    "print(f\"   Student Models: {list(STUDENT_MODELS.keys())}\")  # Display student models\n",
    "\n",
    "print(\"\\nüìä Dataset Configuration:\")  # Dataset section\n",
    "print(f\"   Dataset: {DATASET_NAME}\")  # Display dataset name\n",
    "print(f\"   Total Samples: {TOTAL_SAMPLES:,}\")  # Display sample count\n",
    "print(f\"   Split Ratios: Train={TRAIN_RATIO}, Val={VAL_RATIO}, Test={TEST_RATIO}\")  # Display splits\n",
    "\n",
    "print(\"\\nüîß LoRA Configuration:\")  # LoRA section\n",
    "print(f\"   Rank (r): {LORA_R}\")  # Display LoRA rank\n",
    "print(f\"   Alpha: {LORA_ALPHA}\")  # Display LoRA alpha\n",
    "print(f\"   Dropout: {LORA_DROPOUT}\")  # Display LoRA dropout\n",
    "print(f\"   Target Modules: {LORA_TARGET_MODULES}\")  # Display target modules\n",
    "\n",
    "print(\"\\nüèãÔ∏è Training Configuration:\")  # Training section\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")  # Display epochs\n",
    "print(f\"   Batch Size: {BATCH_SIZE} (Effective: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS})\")  # Display batch size\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")  # Display learning rate\n",
    "print(f\"   Max Sequence Length: {MAX_SEQ_LENGTH}\")  # Display max sequence length\n",
    "\n",
    "print(\"\\nüìè Evaluation Configuration:\")  # Evaluation section\n",
    "print(f\"   Max New Tokens: {MAX_NEW_TOKENS}\")  # Display max new tokens\n",
    "print(f\"   Target Reading Level: Grade {TARGET_READING_LEVEL}\")  # Display target reading level\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line\n",
    "print(\"‚úì All configurations defined successfully\")  # Confirmation message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Define Prompt Templates\n",
    "\n",
    "Define the prompt templates used for text simplification and instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT TEMPLATES DEFINED\n",
      "============================================================\n",
      "\n",
      "üìù Simplification Instruction:\n",
      "----------------------------------------\n",
      "Simplify the following medical discharge summary in plain language for patients with no medical background.\n",
      "Guidelines:\n",
      "- Replace medical jargon with everyday words (e.g., \"hypertension\" ‚Üí \"high blood...\n",
      "\n",
      "üìã Templates Defined:\n",
      "   1. SIMPLIFY_PROMPT_TEMPLATE - For Claude API calls\n",
      "\n",
      "============================================================\n",
      "‚úì All prompt templates defined successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Prompt Templates for Text Simplification\n",
    "# ============================================================================\n",
    "\n",
    "# Instruction for the simplification task (used in all prompts)\n",
    "SIMPLIFICATION_INSTRUCTION = \"\"\"Simplify the following medical discharge summary in plain language for patients with no medical background.\n",
    "Guidelines:\n",
    "- Replace medical jargon with everyday words (e.g., \"hypertension\" ‚Üí \"high blood pressure\")\n",
    "- Keep all important information (diagnoses, medications, follow-up instructions)\n",
    "- Use short, clear sentences (aim for 15-20 words per sentence)\n",
    "- Aim for a 6th-grade reading level\n",
    "- Maintain the same structure as the original\n",
    "- Do not add or omit information\n",
    "- Keep the same patient reference style (e.g., \"The patient\" stays \"The patient\", not \"You\")\n",
    "- Output plain text only (no markdown, no bold, no headers, no bullet points)\n",
    "- Do not include empty lines or separator characters like \"---\" \"\"\"  # Core instruction for simplification\n",
    "\n",
    "# Template for Claude Opus 4.5 API calls (ground truth generation)\n",
    "SIMPLIFY_PROMPT_TEMPLATE = \"\"\"You are a medical communication expert specializing in health literacy. Your task is to simplify medical documents for patients.\n",
    "\n",
    "{instruction}\n",
    "\n",
    "Medical Discharge Summary:\n",
    "---\n",
    "{complex_text}\n",
    "---\n",
    "\n",
    "Simplified Version:\"\"\"  # Template with placeholders for instruction and complex text\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Display Prompt Templates\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"PROMPT TEMPLATES DEFINED\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "print(\"\\nüìù Simplification Instruction:\")  # Section header\n",
    "print(\"-\" * 40)  # Separator\n",
    "print(SIMPLIFICATION_INSTRUCTION[:200] + \"...\")  # Display first 200 chars\n",
    "\n",
    "print(\"\\nüìã Templates Defined:\")  # Section header\n",
    "print(\"   1. SIMPLIFY_PROMPT_TEMPLATE - For Claude API calls\")  # Template 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line\n",
    "print(\"‚úì All prompt templates defined successfully\")  # Confirmation message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 3: Dataset Preparation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Download Asclepius Dataset\n",
    "\n",
    "Load the Asclepius Synthetic Clinical Notes dataset from HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOADING ASCLEPIUS DATASET\n",
      "============================================================\n",
      "\n",
      "üìÅ Dataset cache directory: medisimplifier/HFDatasets\n",
      "\n",
      "üì• Downloading dataset: starmpcc/Asclepius-Synthetic-Clinical-Notes\n",
      "   This may take a few minutes on first run...\n",
      "\n",
      "‚úì Dataset loaded successfully!\n",
      "‚úì Cached at: medisimplifier/HFDatasets\n",
      "\n",
      "üìä Dataset Structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'note', 'question', 'answer', 'task'],\n",
      "        num_rows: 158114\n",
      "    })\n",
      "})\n",
      "\n",
      "üìà Training Split Statistics:\n",
      "   Total samples: 158,114\n",
      "   Features: ['patient_id', 'note', 'question', 'answer', 'task']\n",
      "\n",
      "üìã Sample Entry (first record):\n",
      "----------------------------------------\n",
      "   patient_id: 0\n",
      "   note: Discharge Summary:\n",
      "\n",
      "Patient: 60-year-old male with moderate ARDS from COVID-19\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "The patient was admitted to the hospital with symptoms of fever, dry cough, and dyspnea. During physic...\n",
      "   question: Can you provide a simplified paraphrase of the sentence, 'To avoid rapid deterioration and respiratory failure, a step-by-step approach was used for position changes' in the patient's discharge summar...\n",
      "   answer: The healthcare team used a gradual approach to changing the patient's position to avoid worsening of the respiratory status and prevent respiratory failure.\n",
      "   task: Paraphrasing\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Configure HuggingFace Dataset Cache Directory\n",
    "# ============================================================================\n",
    "\n",
    "import os  # Import os module for environment variables\n",
    "\n",
    "# Configure HuggingFace cache to store datasets in project directory\n",
    "# NOTE: Must be set before calling load_dataset\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = str(HF_DATASETS_DIR)  # Set datasets cache directory\n",
    "os.environ[\"HF_HOME\"] = str(HF_DATASETS_DIR)  # Also set HF_HOME for datasets\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"LOADING ASCLEPIUS DATASET\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "print(f\"\\nüìÅ Dataset cache directory: {HF_DATASETS_DIR}\")  # Show cache path\n",
    "\n",
    "# Load the dataset from HuggingFace Hub\n",
    "print(f\"\\nüì• Downloading dataset: {DATASET_NAME}\")  # Status message\n",
    "print(\"   This may take a few minutes on first run...\")  # Info message\n",
    "\n",
    "# Load the dataset using the datasets library with explicit cache_dir\n",
    "raw_dataset = load_dataset(DATASET_NAME, cache_dir=str(HF_DATASETS_DIR))  # Download to project folder\n",
    "\n",
    "# ============================================================================\n",
    "# Display Dataset Information\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded successfully!\")  # Success message\n",
    "print(f\"‚úì Cached at: {HF_DATASETS_DIR}\")  # Confirm cache location\n",
    "print(f\"\\nüìä Dataset Structure:\")  # Section header\n",
    "print(raw_dataset)  # Display dataset structure\n",
    "\n",
    "# Get the training split (main data source)\n",
    "print(f\"\\nüìà Training Split Statistics:\")  # Section header\n",
    "print(f\"   Total samples: {len(raw_dataset['train']):,}\")  # Display total sample count\n",
    "print(f\"   Features: {list(raw_dataset['train'].features.keys())}\")  # Display feature names\n",
    "\n",
    "# Display a sample entry\n",
    "print(f\"\\nüìã Sample Entry (first record):\")  # Section header\n",
    "print(\"-\" * 40)  # Separator\n",
    "sample = raw_dataset['train'][0]  # Get first sample\n",
    "for key, value in sample.items():  # Iterate through sample fields\n",
    "    # Truncate long values for display\n",
    "    display_value = str(value)[:200] + \"...\" if len(str(value)) > 200 else str(value)  # Truncate if needed\n",
    "    print(f\"   {key}: {display_value}\")  # Display field\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Sample and Filter Dataset\n",
    "\n",
    "Shuffle the dataset, select the target number of samples, and filter out notes that are too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLING AND FILTERING DATASET\n",
      "============================================================\n",
      "\n",
      "üìä Original dataset size: 158,114 samples\n",
      "\n",
      "üîÄ Shuffling dataset with seed=42...\n",
      "\n",
      "‚úÇÔ∏è  Selecting first 10,000 samples...\n",
      "   Sampled size: 10,000 samples\n",
      "\n",
      "üîç Filtering notes shorter than 200 characters...\n",
      "   Using column 'note' as text column\n",
      "\n",
      "üìä Filtering Statistics:\n",
      "   Samples before filtering: 10,000\n",
      "   Samples after filtering: 10,000\n",
      "   Samples removed: 0 (0.0%)\n",
      "\n",
      "‚úì Dataset sampled and filtered successfully\n",
      "   Final dataset size: 10,000 samples\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Sample and Filter Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"SAMPLING AND FILTERING DATASET\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "# Get the training split for processing\n",
    "full_train = raw_dataset['train']  # Extract training split\n",
    "print(f\"\\nüìä Original dataset size: {len(full_train):,} samples\")  # Display original size\n",
    "\n",
    "# Shuffle the dataset with a fixed seed for reproducibility\n",
    "print(f\"\\nüîÄ Shuffling dataset with seed={SEED}...\")  # Status message\n",
    "shuffled_dataset = full_train.shuffle(seed=SEED)  # Shuffle with fixed seed\n",
    "\n",
    "# Select the first TOTAL_SAMPLES from the shuffled dataset\n",
    "print(f\"\\n‚úÇÔ∏è  Selecting first {TOTAL_SAMPLES:,} samples...\")  # Status message\n",
    "sampled_dataset = shuffled_dataset.select(range(min(TOTAL_SAMPLES, len(shuffled_dataset))))  # Select samples\n",
    "print(f\"   Sampled size: {len(sampled_dataset):,} samples\")  # Display sampled size\n",
    "\n",
    "# ============================================================================\n",
    "# Filter Notes by Minimum Length\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîç Filtering notes shorter than {MIN_NOTE_LENGTH} characters...\")  # Status message\n",
    "\n",
    "# Determine the text column name (may vary by dataset)\n",
    "# Check for common column names\n",
    "text_column = None  # Initialize text column variable\n",
    "for col_name in ['note', 'text', 'clinical_note', 'content', 'input']:  # Check common names\n",
    "    if col_name in sampled_dataset.column_names:  # Check if column exists\n",
    "        text_column = col_name  # Set text column\n",
    "        break  # Exit loop\n",
    "\n",
    "# If no common name found, use the first column\n",
    "if text_column is None:  # If no match found\n",
    "    text_column = sampled_dataset.column_names[0]  # Use first column\n",
    "    print(f\"   ‚ö†Ô∏è Using column '{text_column}' as text column\")  # Warning\n",
    "else:  # Match found\n",
    "    print(f\"   Using column '{text_column}' as text column\")  # Info message\n",
    "\n",
    "# Define filter function for minimum length\n",
    "def filter_by_length(example):  # Filter function\n",
    "    \"\"\"Filter out examples with text shorter than MIN_NOTE_LENGTH.\"\"\"\n",
    "    text = example[text_column]  # Get text from example\n",
    "    return len(str(text)) >= MIN_NOTE_LENGTH  # Return True if long enough\n",
    "\n",
    "# Count samples before filtering\n",
    "samples_before = len(sampled_dataset)  # Store count before filtering\n",
    "\n",
    "# Apply the filter\n",
    "filtered_dataset = sampled_dataset.filter(filter_by_length)  # Filter short notes\n",
    "\n",
    "# Calculate filtering statistics\n",
    "samples_after = len(filtered_dataset)  # Count after filtering\n",
    "samples_removed = samples_before - samples_after  # Calculate removed count\n",
    "removal_percentage = (samples_removed / samples_before) * 100  # Calculate percentage\n",
    "\n",
    "# ============================================================================\n",
    "# Display Filtering Statistics\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Filtering Statistics:\")  # Section header\n",
    "print(f\"   Samples before filtering: {samples_before:,}\")  # Before count\n",
    "print(f\"   Samples after filtering: {samples_after:,}\")  # After count\n",
    "print(f\"   Samples removed: {samples_removed:,} ({removal_percentage:.1f}%)\")  # Removed count\n",
    "\n",
    "# Store the text column name for later use\n",
    "TEXT_COLUMN = text_column  # Store in global variable\n",
    "\n",
    "print(f\"\\n‚úì Dataset sampled and filtered successfully\")  # Success message\n",
    "print(f\"   Final dataset size: {len(filtered_dataset):,} samples\")  # Final size\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Analyze Dataset Statistics\n",
    "\n",
    "Calculate and visualize statistics about the clinical note lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET STATISTICS ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä Calculating note length statistics...\n",
      "\n",
      "üìà Note Length Statistics (characters):\n",
      "   Minimum: 779\n",
      "   Maximum: 5,208\n",
      "   Mean: 1,932.7\n",
      "   Median: 1,870.0\n",
      "   Std Dev: 558.0\n",
      "   25th Percentile: 1,529.0\n",
      "   75th Percentile: 2,259.0\n",
      "\n",
      "üìä Generating note length histogram...\n",
      "   ‚úì Saved to: medisimplifier/results/figures/note_length_distribution.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Calculate Dataset Statistics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"DATASET STATISTICS ANALYSIS\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "# Calculate note lengths for all samples in the training set\n",
    "print(\"\\nüìä Calculating note length statistics...\")  # Status message\n",
    "\n",
    "# Extract all text lengths from the training set\n",
    "all_lengths = [len(str(note)) for note in filtered_dataset[TEXT_COLUMN]]\n",
    "\n",
    "# Calculate statistics using numpy\n",
    "length_stats = {  # Dictionary of statistics\n",
    "    'min': int(np.min(all_lengths)),  # Minimum length\n",
    "    'max': int(np.max(all_lengths)),  # Maximum length\n",
    "    'mean': float(np.mean(all_lengths)),  # Mean length\n",
    "    'median': float(np.median(all_lengths)),  # Median length\n",
    "    'std': float(np.std(all_lengths)),  # Standard deviation\n",
    "    'q25': float(np.percentile(all_lengths, 25)),  # 25th percentile\n",
    "    'q75': float(np.percentile(all_lengths, 75)),  # 75th percentile\n",
    "}\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nüìà Note Length Statistics (characters):\")\n",
    "print(f\"   Minimum: {length_stats['min']:,}\")  # Display min\n",
    "print(f\"   Maximum: {length_stats['max']:,}\")  # Display max\n",
    "print(f\"   Mean: {length_stats['mean']:,.1f}\")  # Display mean\n",
    "print(f\"   Median: {length_stats['median']:,.1f}\")  # Display median\n",
    "print(f\"   Std Dev: {length_stats['std']:,.1f}\")  # Display std dev\n",
    "print(f\"   25th Percentile: {length_stats['q25']:,.1f}\")  # Display Q1\n",
    "print(f\"   75th Percentile: {length_stats['q75']:,.1f}\")  # Display Q3\n",
    "\n",
    "# ============================================================================\n",
    "# Create Histogram Visualization\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Generating note length histogram...\")  # Status message\n",
    "\n",
    "# Set up the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Create figure with specified size\n",
    "\n",
    "# Create histogram\n",
    "ax.hist(all_lengths, bins=50, color='steelblue', edgecolor='white', alpha=0.7)  # Plot histogram\n",
    "\n",
    "# Add vertical lines for mean and median\n",
    "ax.axvline(length_stats['mean'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {length_stats['mean']:,.0f}\")  # Mean line\n",
    "ax.axvline(length_stats['median'], color='green', linestyle='--', linewidth=2, label=f\"Median: {length_stats['median']:,.0f}\")  # Median line\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Note Length (characters)', fontsize=12)  # X-axis label\n",
    "ax.set_ylabel('Frequency', fontsize=12)  # Y-axis label\n",
    "ax.set_title('Distribution of Clinical Note Lengths', fontsize=14, fontweight='bold')  # Title\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right', fontsize=10)  # Add legend\n",
    "\n",
    "# Add grid for readability\n",
    "ax.grid(axis='y', alpha=0.3)  # Add horizontal grid lines\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()  # Adjust spacing\n",
    "\n",
    "# Save figure\n",
    "fig_path = FIGURES_DIR / \"note_length_distribution.png\"  # Define save path\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches='tight')  # Save figure\n",
    "print(f\"   ‚úì Saved to: {fig_path}\")  # Confirmation\n",
    "\n",
    "# Display the figure\n",
    "plt.show()  # Show the plot\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 4: Ground Truth Generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define Ground Truth Generation Function\n",
    "\n",
    "Create a function to generate simplified text using Claude Opus 4.5 API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ground truth generation function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Define Ground Truth Generation Function\n",
    "# ============================================================================\n",
    "\n",
    "import time  # Import time module for delays\n",
    "\n",
    "def generate_simplified_text(\n",
    "    complex_text: str,  # The complex medical text to simplify\n",
    "    client: anthropic.Anthropic,  # Anthropic API client\n",
    "    max_retries: int = 3,  # Maximum number of retry attempts\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Generate simplified text using Claude Opus 4.5 API.\"\"\"\n",
    "    \n",
    "    # Format the prompt using the template\n",
    "    prompt = SIMPLIFY_PROMPT_TEMPLATE.format(  # Format prompt with template\n",
    "        instruction=SIMPLIFICATION_INSTRUCTION,  # Simplification guidelines\n",
    "        complex_text=complex_text  # The text to simplify\n",
    "    )\n",
    "    \n",
    "    # Retry loop with exponential backoff\n",
    "    for attempt in range(max_retries):  # Loop through retry attempts\n",
    "        try:  # Try to make API call\n",
    "            # Make API call to Claude\n",
    "            response = client.messages.create(  # Create message\n",
    "                model=TEACHER_MODEL,  # Use Claude Opus 4.5\n",
    "                max_tokens=API_MAX_TOKENS,  # Maximum response tokens\n",
    "                messages=[  # Messages list\n",
    "                    {  # User message\n",
    "                        \"role\": \"user\",  # Role is user\n",
    "                        \"content\": prompt  # The prompt\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Extract and return the simplified text\n",
    "            simplified_text = response.content[0].text  # Get text from response\n",
    "            return simplified_text.strip()  # Return stripped text\n",
    "            \n",
    "        except anthropic.RateLimitError as e:  # Handle rate limit errors\n",
    "            wait_time = API_RATE_LIMIT_DELAY * (2 ** attempt)  # Exponential backoff\n",
    "            print(f\"      ‚ö†Ô∏è Rate limit hit, waiting {wait_time}s (attempt {attempt + 1}/{max_retries})\")  # Warning\n",
    "            time.sleep(wait_time)  # Wait before retry\n",
    "            \n",
    "        except anthropic.APIError as e:  # Handle other API errors\n",
    "            print(f\"      ‚ùå API error: {e}\")  # Print error\n",
    "            if attempt < max_retries - 1:  # If not last attempt\n",
    "                time.sleep(API_RATE_LIMIT_DELAY)  # Wait before retry\n",
    "            else:  # Last attempt failed\n",
    "                return None  # Return None to indicate failure\n",
    "                \n",
    "        except Exception as e:  # Handle unexpected errors\n",
    "            print(f\"      ‚ùå Unexpected error: {e}\")  # Print error\n",
    "            return None  # Return None to indicate failure\n",
    "    \n",
    "    return None  # Return None if all retries failed\n",
    "\n",
    "print(\"‚úì Ground truth generation function defined\")  # Confirmation message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Load or Initialize Checkpoint\n",
    "\n",
    "Load existing checkpoint if available, or initialize a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKPOINT MANAGEMENT\n",
      "============================================================\n",
      "\n",
      "üìÇ Found existing checkpoint: medisimplifier/checkpoints/ground_truth_checkpoint.json\n",
      "   ‚úì Loaded 10020 existing labels\n",
      "   ‚úì 10020 samples already processed\n",
      "\n",
      "üìä Checkpoint status:\n",
      "   - Labels generated: 10020\n",
      "   - Indices processed: 10020\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Load or Initialize Checkpoint\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"CHECKPOINT MANAGEMENT\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "# Define checkpoint file path\n",
    "checkpoint_path = CHECKPOINTS_DIR / \"ground_truth_checkpoint.json\"  # Path for checkpoint file\n",
    "\n",
    "# Try to load existing checkpoint\n",
    "if checkpoint_path.exists():  # Check if checkpoint exists\n",
    "    print(f\"\\nüìÇ Found existing checkpoint: {checkpoint_path}\")  # Status message\n",
    "    with open(checkpoint_path, 'r') as f:  # Open checkpoint file\n",
    "        checkpoint_data = json.load(f)  # Load checkpoint data\n",
    "    ground_truth_labels = checkpoint_data.get(\"labels\", [])  # Get existing labels\n",
    "    processed_indices = set(checkpoint_data.get(\"processed_indices\", []))  # Get processed indices\n",
    "    print(f\"   ‚úì Loaded {len(ground_truth_labels)} existing labels\")  # Confirmation\n",
    "    print(f\"   ‚úì {len(processed_indices)} samples already processed\")  # Status\n",
    "else:  # No existing checkpoint\n",
    "    print(f\"\\nüìÇ No existing checkpoint found\")  # Status message\n",
    "    print(f\"   Initializing new checkpoint...\")  # Status message\n",
    "    ground_truth_labels = []  # Initialize empty labels list\n",
    "    processed_indices = set()  # Initialize empty processed set\n",
    "    print(f\"   ‚úì Initialized new checkpoint\")  # Confirmation\n",
    "\n",
    "print(f\"\\nüìä Checkpoint status:\")  # Section header\n",
    "print(f\"   - Labels generated: {len(ground_truth_labels)}\")  # Label count\n",
    "print(f\"   - Indices processed: {len(processed_indices)}\")  # Processed count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Generate Ground Truth Labels\n",
    "\n",
    "Generate simplified texts using Claude Opus 4.5 for all training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING GROUND TRUTH LABELS\n",
      "============================================================\n",
      "\n",
      "üìä Samples to process:\n",
      "   Train: 8,000\n",
      "   Validation: 1,000\n",
      "   Test: 1,000\n",
      "   Total: 10,000\n",
      "\n",
      "üìä Already processed: 3,446\n",
      "üìä Remaining: 6,554\n",
      "\n",
      "üîß API Configuration:\n",
      "   Model: claude-opus-4-5-20251101\n",
      "   Max Tokens: 1024\n",
      "   Rate Limit Delay: 1.0s\n",
      "   Checkpoint Interval: 100\n",
      "\n",
      "üîå Testing API connection...\n",
      "   ‚úì API test successful: OK\n",
      "   ‚úì Model confirmed: claude-opus-4-5-20251101\n",
      "\n",
      "üîÑ Starting generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6264419ee3e44449e79eb0e50140758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   üì§ API Request [1]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3143 chars\n",
      "   üì• API Response:\n",
      "      Duration: 3.21s\n",
      "      Tokens: 729 in / 48 out\n",
      "\n",
      "   ‚ö†Ô∏è train_1854: Empty response from API\n",
      "\n",
      "   üíæ Checkpoint: 3500 samples (10.0 min)\n",
      "      Train: 54, Val: 0, Test: 0\n",
      "\n",
      "   [train:3408] Processing (1942 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 3600 samples (29.3 min)\n",
      "      Train: 154, Val: 0, Test: 0\n",
      "\n",
      "   [train:3508] Processing (1967 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 3700 samples (50.4 min)\n",
      "      Train: 254, Val: 0, Test: 0\n",
      "\n",
      "   [train:3724] Processing (2309 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 3800 samples (69.9 min)\n",
      "      Train: 354, Val: 0, Test: 0\n",
      "\n",
      "   [train:3824] Processing (1380 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 3900 samples (88.8 min)\n",
      "      Train: 454, Val: 0, Test: 0\n",
      "\n",
      "   [train:3924] Processing (2732 chars)...\n",
      "\n",
      "   üì§ API Request [500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3628 chars\n",
      "   üì• API Response:\n",
      "      Duration: 10.67s\n",
      "      Tokens: 845 in / 575 out\n",
      "\n",
      "   üíæ Checkpoint: 4000 samples (112.4 min)\n",
      "      Train: 554, Val: 0, Test: 0\n",
      "\n",
      "   [train:4024] Processing (2047 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4100 samples (131.6 min)\n",
      "      Train: 654, Val: 0, Test: 0\n",
      "\n",
      "   [train:4124] Processing (1861 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4200 samples (150.7 min)\n",
      "      Train: 754, Val: 0, Test: 0\n",
      "\n",
      "   [train:4224] Processing (2649 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4300 samples (170.1 min)\n",
      "      Train: 854, Val: 0, Test: 0\n",
      "\n",
      "   [train:4324] Processing (1562 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4400 samples (188.8 min)\n",
      "      Train: 954, Val: 0, Test: 0\n",
      "\n",
      "   [train:4424] Processing (1506 chars)...\n",
      "\n",
      "   üì§ API Request [1000]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 2402 chars\n",
      "   üì• API Response:\n",
      "      Duration: 10.39s\n",
      "      Tokens: 557 in / 401 out\n",
      "\n",
      "   üíæ Checkpoint: 4500 samples (208.2 min)\n",
      "      Train: 1054, Val: 0, Test: 0\n",
      "\n",
      "   [train:4524] Processing (2514 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4600 samples (227.0 min)\n",
      "      Train: 1154, Val: 0, Test: 0\n",
      "\n",
      "   [train:4624] Processing (1396 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4700 samples (245.8 min)\n",
      "      Train: 1254, Val: 0, Test: 0\n",
      "\n",
      "   [train:4724] Processing (1597 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4800 samples (265.0 min)\n",
      "      Train: 1354, Val: 0, Test: 0\n",
      "\n",
      "   [train:4824] Processing (2454 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 4900 samples (284.4 min)\n",
      "      Train: 1454, Val: 0, Test: 0\n",
      "\n",
      "   [train:4924] Processing (2237 chars)...\n",
      "\n",
      "   üì§ API Request [1500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3133 chars\n",
      "   üì• API Response:\n",
      "      Duration: 10.41s\n",
      "      Tokens: 767 in / 522 out\n",
      "\n",
      "   üíæ Checkpoint: 5000 samples (304.5 min)\n",
      "      Train: 1554, Val: 0, Test: 0\n",
      "\n",
      "   [train:5024] Processing (1809 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5100 samples (323.2 min)\n",
      "      Train: 1654, Val: 0, Test: 0\n",
      "\n",
      "   [train:5124] Processing (1611 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5200 samples (342.6 min)\n",
      "      Train: 1754, Val: 0, Test: 0\n",
      "\n",
      "   [train:5224] Processing (2060 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5300 samples (361.2 min)\n",
      "      Train: 1854, Val: 0, Test: 0\n",
      "\n",
      "   [train:5324] Processing (2222 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5400 samples (380.4 min)\n",
      "      Train: 1954, Val: 0, Test: 0\n",
      "\n",
      "   [train:5424] Processing (1246 chars)...\n",
      "\n",
      "   üì§ API Request [2000]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 2142 chars\n",
      "   üì• API Response:\n",
      "      Duration: 7.70s\n",
      "      Tokens: 522 in / 316 out\n",
      "\n",
      "   üíæ Checkpoint: 5500 samples (399.1 min)\n",
      "      Train: 2054, Val: 0, Test: 0\n",
      "\n",
      "   [train:5524] Processing (2007 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5600 samples (417.9 min)\n",
      "      Train: 2154, Val: 0, Test: 0\n",
      "\n",
      "   [train:5624] Processing (1722 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5700 samples (436.4 min)\n",
      "      Train: 2254, Val: 0, Test: 0\n",
      "\n",
      "   [train:5724] Processing (2118 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5800 samples (455.5 min)\n",
      "      Train: 2354, Val: 0, Test: 0\n",
      "\n",
      "   [train:5824] Processing (2405 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 5900 samples (476.8 min)\n",
      "      Train: 2454, Val: 0, Test: 0\n",
      "\n",
      "   [train:5924] Processing (1783 chars)...\n",
      "\n",
      "   üì§ API Request [2500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 2679 chars\n",
      "   üì• API Response:\n",
      "      Duration: 10.73s\n",
      "      Tokens: 634 in / 424 out\n",
      "\n",
      "   üíæ Checkpoint: 6000 samples (495.8 min)\n",
      "      Train: 2554, Val: 0, Test: 0\n",
      "\n",
      "   [train:6024] Processing (1849 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6100 samples (520.2 min)\n",
      "      Train: 2654, Val: 0, Test: 0\n",
      "\n",
      "   [train:6124] Processing (1527 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6200 samples (538.4 min)\n",
      "      Train: 2754, Val: 0, Test: 0\n",
      "\n",
      "   [train:6224] Processing (1675 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6300 samples (557.3 min)\n",
      "      Train: 2854, Val: 0, Test: 0\n",
      "\n",
      "   [train:6324] Processing (2441 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6400 samples (575.4 min)\n",
      "      Train: 2954, Val: 0, Test: 0\n",
      "\n",
      "   [train:6424] Processing (2112 chars)...\n",
      "\n",
      "   üì§ API Request [3000]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3008 chars\n",
      "   üì• API Response:\n",
      "      Duration: 9.16s\n",
      "      Tokens: 704 in / 468 out\n",
      "\n",
      "   üíæ Checkpoint: 6500 samples (594.3 min)\n",
      "      Train: 3054, Val: 0, Test: 0\n",
      "\n",
      "   [train:6524] Processing (2008 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6600 samples (613.7 min)\n",
      "      Train: 3154, Val: 0, Test: 0\n",
      "\n",
      "   [train:6624] Processing (1258 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6700 samples (632.4 min)\n",
      "      Train: 3254, Val: 0, Test: 0\n",
      "\n",
      "   [train:6724] Processing (2462 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6800 samples (651.0 min)\n",
      "      Train: 3354, Val: 0, Test: 0\n",
      "\n",
      "   [train:6824] Processing (1231 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 6900 samples (670.1 min)\n",
      "      Train: 3454, Val: 0, Test: 0\n",
      "\n",
      "   [train:6924] Processing (1726 chars)...\n",
      "\n",
      "   üì§ API Request [3500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 2622 chars\n",
      "   üì• API Response:\n",
      "      Duration: 10.19s\n",
      "      Tokens: 627 in / 446 out\n",
      "\n",
      "   üíæ Checkpoint: 7000 samples (689.4 min)\n",
      "      Train: 3554, Val: 0, Test: 0\n",
      "\n",
      "   [train:7024] Processing (957 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7100 samples (708.9 min)\n",
      "      Train: 3654, Val: 0, Test: 0\n",
      "\n",
      "   [train:7124] Processing (1134 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7200 samples (727.7 min)\n",
      "      Train: 3754, Val: 0, Test: 0\n",
      "\n",
      "   [train:7224] Processing (3438 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7300 samples (746.7 min)\n",
      "      Train: 3854, Val: 0, Test: 0\n",
      "\n",
      "   [train:7324] Processing (1813 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7400 samples (767.8 min)\n",
      "      Train: 3954, Val: 0, Test: 0\n",
      "\n",
      "   [train:7424] Processing (2613 chars)...\n",
      "\n",
      "   üì§ API Request [4000]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3509 chars\n",
      "   üì• API Response:\n",
      "      Duration: 12.03s\n",
      "      Tokens: 968 in / 692 out\n",
      "\n",
      "   üíæ Checkpoint: 7500 samples (787.0 min)\n",
      "      Train: 4054, Val: 0, Test: 0\n",
      "\n",
      "   [train:7524] Processing (3517 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7600 samples (805.9 min)\n",
      "      Train: 4154, Val: 0, Test: 0\n",
      "\n",
      "   [train:7624] Processing (1595 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7700 samples (824.5 min)\n",
      "      Train: 4254, Val: 0, Test: 0\n",
      "\n",
      "   [train:7724] Processing (1339 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7800 samples (844.1 min)\n",
      "      Train: 4354, Val: 0, Test: 0\n",
      "\n",
      "   [train:7824] Processing (2391 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 7900 samples (864.1 min)\n",
      "      Train: 4454, Val: 0, Test: 0\n",
      "\n",
      "   [train:7924] Processing (1056 chars)...\n",
      "\n",
      "   üì§ API Request [4500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 1952 chars\n",
      "   üì• API Response:\n",
      "      Duration: 7.22s\n",
      "      Tokens: 500 in / 262 out\n",
      "\n",
      "   üíæ Checkpoint: 8000 samples (883.0 min)\n",
      "      Train: 4554, Val: 0, Test: 0\n",
      "\n",
      "   [validation:24] Processing (956 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8100 samples (901.9 min)\n",
      "      Train: 4574, Val: 80, Test: 0\n",
      "\n",
      "   [validation:124] Processing (1683 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8200 samples (920.7 min)\n",
      "      Train: 4574, Val: 180, Test: 0\n",
      "\n",
      "   [validation:224] Processing (2311 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8300 samples (939.3 min)\n",
      "      Train: 4574, Val: 280, Test: 0\n",
      "\n",
      "   [validation:324] Processing (2521 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8400 samples (959.2 min)\n",
      "      Train: 4574, Val: 380, Test: 0\n",
      "\n",
      "   [validation:424] Processing (2521 chars)...\n",
      "\n",
      "   üì§ API Request [5000]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3417 chars\n",
      "   üì• API Response:\n",
      "      Duration: 12.55s\n",
      "      Tokens: 845 in / 581 out\n",
      "\n",
      "   üíæ Checkpoint: 8500 samples (978.3 min)\n",
      "      Train: 4574, Val: 480, Test: 0\n",
      "\n",
      "   [validation:524] Processing (1889 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8600 samples (997.0 min)\n",
      "      Train: 4574, Val: 580, Test: 0\n",
      "\n",
      "   [validation:624] Processing (1847 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8700 samples (1015.6 min)\n",
      "      Train: 4574, Val: 680, Test: 0\n",
      "\n",
      "   [validation:724] Processing (1849 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8800 samples (1034.2 min)\n",
      "      Train: 4574, Val: 780, Test: 0\n",
      "\n",
      "   [validation:824] Processing (1321 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 8900 samples (1052.7 min)\n",
      "      Train: 4574, Val: 880, Test: 0\n",
      "\n",
      "   [validation:924] Processing (1470 chars)...\n",
      "\n",
      "   üì§ API Request [5500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 2366 chars\n",
      "   üì• API Response:\n",
      "      Duration: 8.61s\n",
      "      Tokens: 570 in / 380 out\n",
      "\n",
      "   üíæ Checkpoint: 9000 samples (1072.1 min)\n",
      "      Train: 4574, Val: 980, Test: 0\n",
      "\n",
      "   [test:24] Processing (1946 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9100 samples (1090.9 min)\n",
      "      Train: 4574, Val: 1000, Test: 80\n",
      "\n",
      "   [test:124] Processing (2112 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9200 samples (1109.6 min)\n",
      "      Train: 4574, Val: 1000, Test: 180\n",
      "\n",
      "   [test:224] Processing (1601 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9300 samples (1128.7 min)\n",
      "      Train: 4574, Val: 1000, Test: 280\n",
      "\n",
      "   [test:324] Processing (2534 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9400 samples (1148.1 min)\n",
      "      Train: 4574, Val: 1000, Test: 380\n",
      "\n",
      "   [test:424] Processing (1201 chars)...\n",
      "\n",
      "   üì§ API Request [6000]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 2097 chars\n",
      "   üì• API Response:\n",
      "      Duration: 8.83s\n",
      "      Tokens: 502 in / 385 out\n",
      "\n",
      "   üíæ Checkpoint: 9500 samples (1167.0 min)\n",
      "      Train: 4574, Val: 1000, Test: 480\n",
      "\n",
      "   [test:524] Processing (1598 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9600 samples (1185.4 min)\n",
      "      Train: 4574, Val: 1000, Test: 580\n",
      "\n",
      "   [test:624] Processing (2709 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9700 samples (1203.6 min)\n",
      "      Train: 4574, Val: 1000, Test: 680\n",
      "\n",
      "   [test:724] Processing (2508 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9800 samples (1222.2 min)\n",
      "      Train: 4574, Val: 1000, Test: 780\n",
      "\n",
      "   [test:824] Processing (1913 chars)...\n",
      "\n",
      "   üíæ Checkpoint: 9900 samples (1240.6 min)\n",
      "      Train: 4574, Val: 1000, Test: 880\n",
      "\n",
      "   [test:924] Processing (2117 chars)...\n",
      "\n",
      "   üì§ API Request [6500]:\n",
      "      Model: claude-opus-4-5-20251101\n",
      "      Prompt Length: 3013 chars\n",
      "   üì• API Response:\n",
      "      Duration: 10.39s\n",
      "      Tokens: 694 in / 463 out\n",
      "\n",
      "   üíæ Checkpoint: 10000 samples (1259.7 min)\n",
      "      Train: 4574, Val: 1000, Test: 980\n",
      "\n",
      "============================================================\n",
      "GENERATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üìä Final Statistics:\n",
      "   Total Attempted: 6575\n",
      "   Successful: 6574\n",
      "   Failed: 1\n",
      "   Success Rate: 100.0%\n",
      "\n",
      "üìä By Split:\n",
      "   Train: 4574\n",
      "   Validation: 1000\n",
      "   Test: 1000\n",
      "\n",
      "‚è±Ô∏è Duration: 1263.4 minutes\n",
      "   Avg: 11.53 sec/sample\n",
      "\n",
      "‚ö†Ô∏è Errors (1):\n",
      "   - train_1854: Empty response from API\n",
      "\n",
      "‚úì Final checkpoint: medisimplifier/checkpoints/ground_truth_checkpoint.json\n",
      "‚úì Total ground truth labels: 10020\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Generate Ground Truth Labels for Entire Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"GENERATING GROUND TRUTH LABELS\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic()  # Create client using ANTHROPIC_API_KEY env var\n",
    "\n",
    "# Combine all splits: Train (8K) + Validation (1K) + Test (1K) = 10K\n",
    "all_texts_to_process = []  # Initialize list\n",
    "\n",
    "for split_name in ['train', 'validation', 'test']:  # Loop through splits\n",
    "    split_texts = dataset_splits[split_name][TEXT_COLUMN]  # Get texts for split\n",
    "    for i, text in enumerate(split_texts):  # Loop through texts\n",
    "        all_texts_to_process.append((split_name, i, text))  # Add tuple (split, index, text)\n",
    "\n",
    "print(f\"\\nüìä Samples to process:\")  # Display counts\n",
    "print(f\"   Train: {len(dataset_splits['train']):,}\")  # Training count\n",
    "print(f\"   Validation: {len(dataset_splits['validation']):,}\")  # Validation count\n",
    "print(f\"   Test: {len(dataset_splits['test']):,}\")  # Test count\n",
    "print(f\"   Total: {len(all_texts_to_process):,}\")  # Total count\n",
    "print(f\"\\nüìä Already processed: {len(ground_truth_labels):,}\")  # Display checkpoint count\n",
    "print(f\"üìä Remaining: {len(all_texts_to_process) - len(processed_indices):,}\")  # Display remaining\n",
    "\n",
    "# Log API configuration\n",
    "print(f\"\\nüîß API Configuration:\")  # Section header\n",
    "print(f\"   Model: {TEACHER_MODEL}\")  # Show model being used\n",
    "print(f\"   Max Tokens: {API_MAX_TOKENS}\")  # Show max tokens\n",
    "print(f\"   Rate Limit Delay: {API_RATE_LIMIT_DELAY}s\")  # Show delay\n",
    "print(f\"   Checkpoint Interval: {CHECKPOINT_INTERVAL}\")  # Show checkpoint interval\n",
    "\n",
    "# Test API connection before starting\n",
    "print(f\"\\nüîå Testing API connection...\")  # Status message\n",
    "try:\n",
    "    test_response = client.messages.create(  # Test API call\n",
    "        model=TEACHER_MODEL,  # Use configured model\n",
    "        max_tokens=10,  # Minimal tokens for test\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say OK\"}]  # Simple test\n",
    "    )\n",
    "    print(f\"   ‚úì API test successful: {test_response.content[0].text}\")  # Confirmation\n",
    "    print(f\"   ‚úì Model confirmed: {TEACHER_MODEL}\")  # Show model\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå API test failed: {type(e).__name__}: {e}\")  # Error details\n",
    "    raise  # Stop execution if API fails\n",
    "\n",
    "# Track statistics\n",
    "stats = {  # Statistics dictionary\n",
    "    \"total_attempted\": 0,  # Total API calls attempted\n",
    "    \"successful\": 0,  # Successful calls\n",
    "    \"failed\": 0,  # Failed calls\n",
    "    \"errors\": [],  # List of errors\n",
    "    \"by_split\": {\"train\": 0, \"validation\": 0, \"test\": 0},  # Count per split\n",
    "}\n",
    "\n",
    "# Generate simplified texts\n",
    "print(f\"\\nüîÑ Starting generation...\")  # Status message\n",
    "start_time = datetime.now()  # Record start time\n",
    "\n",
    "for split, idx, complex_text in tqdm(all_texts_to_process, desc=\"Generating\"):  # Loop through all\n",
    "    # Create unique key for this sample\n",
    "    sample_key = f\"{split}_{idx}\"  # Unique identifier\n",
    "    \n",
    "    # Skip if already processed\n",
    "    if sample_key in processed_indices:  # Check if already done\n",
    "        continue  # Skip to next\n",
    "    \n",
    "    stats[\"total_attempted\"] += 1  # Increment attempt counter\n",
    "    \n",
    "    # Log every 100th sample for debugging\n",
    "    if stats[\"total_attempted\"] % 100 == 0:  # Every 100 samples\n",
    "        print(f\"\\n   [{split}:{idx}] Processing ({len(complex_text)} chars)...\")  # Log progress\n",
    "    \n",
    "    try:\n",
    "        # Record API call start time\n",
    "        api_start = datetime.now()  # Record API call start\n",
    "        \n",
    "        # Format the prompt\n",
    "        prompt = SIMPLIFY_PROMPT_TEMPLATE.format(  # Create prompt\n",
    "            instruction=SIMPLIFICATION_INSTRUCTION,  # Add instruction\n",
    "            complex_text=complex_text  # Add medical text\n",
    "        )\n",
    "        \n",
    "        # Log API request details (first call only or every 500)\n",
    "        if stats[\"total_attempted\"] == 1 or stats[\"total_attempted\"] % 500 == 0:  # First or every 500\n",
    "            print(f\"\\n   üì§ API Request [{stats['total_attempted']}]:\")  # Header\n",
    "            print(f\"      Model: {TEACHER_MODEL}\")  # Model\n",
    "            print(f\"      Prompt Length: {len(prompt)} chars\")  # Prompt length\n",
    "        \n",
    "        # Make the API call\n",
    "        response = client.messages.create(  # API call\n",
    "            model=TEACHER_MODEL,  # Model to use\n",
    "            max_tokens=API_MAX_TOKENS,  # Maximum tokens\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]  # Message\n",
    "        )\n",
    "        \n",
    "        api_duration = (datetime.now() - api_start).total_seconds()  # Calculate duration\n",
    "        \n",
    "        # Extract simplified text with safety check\n",
    "        if response.content and len(response.content) > 0:  # Check if content exists\n",
    "            simplified_text = response.content[0].text.strip()  # Get response text\n",
    "        else:  # Empty response\n",
    "            error_msg = f\"{sample_key}: Empty response from API\"  # Format error\n",
    "            print(f\"\\n   ‚ö†Ô∏è {error_msg}\")  # Log warning\n",
    "            stats[\"failed\"] += 1  # Increment failure counter\n",
    "            stats[\"errors\"].append(error_msg)  # Store error\n",
    "            continue  # Skip to next sample\n",
    "        \n",
    "        # Log API response details (first call only or every 500)\n",
    "        if stats[\"total_attempted\"] == 1 or stats[\"total_attempted\"] % 500 == 0:  # First or every 500\n",
    "            print(f\"   üì• API Response:\")  # Header\n",
    "            print(f\"      Duration: {api_duration:.2f}s\")  # Duration\n",
    "            print(f\"      Tokens: {response.usage.input_tokens} in / {response.usage.output_tokens} out\")  # Tokens\n",
    "        \n",
    "        # Store the result\n",
    "        ground_truth_labels.append({  # Add to results\n",
    "            \"split\": split,  # Which split (train/validation/test)\n",
    "            \"index\": idx,  # Sample index within split\n",
    "            \"sample_key\": sample_key,  # Unique key\n",
    "            \"complex_text\": complex_text,  # Original text\n",
    "            \"simplified_text\": simplified_text,  # Simplified text\n",
    "            \"input_tokens\": response.usage.input_tokens,  # Tokens used\n",
    "            \"output_tokens\": response.usage.output_tokens,  # Tokens generated\n",
    "        })\n",
    "        \n",
    "        processed_indices.add(sample_key)  # Mark as processed\n",
    "        stats[\"successful\"] += 1  # Increment success counter\n",
    "        stats[\"by_split\"][split] += 1  # Increment split counter\n",
    "        \n",
    "    except anthropic.APIError as e:  # API-specific errors\n",
    "        error_msg = f\"{sample_key}: {type(e).__name__}: {e}\"  # Format error\n",
    "        print(f\"\\n   ‚ùå {error_msg}\")  # Log error\n",
    "        stats[\"failed\"] += 1  # Increment failure counter\n",
    "        stats[\"errors\"].append(error_msg)  # Store error\n",
    "        \n",
    "    except Exception as e:  # General errors\n",
    "        error_msg = f\"{sample_key}: {type(e).__name__}: {e}\"  # Format error\n",
    "        print(f\"\\n   ‚ùå {error_msg}\")  # Log error\n",
    "        stats[\"failed\"] += 1  # Increment failure counter\n",
    "        stats[\"errors\"].append(error_msg)  # Store error\n",
    "    \n",
    "    # Rate limiting delay\n",
    "    time.sleep(API_RATE_LIMIT_DELAY)  # Wait to avoid rate limits\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if len(ground_truth_labels) % CHECKPOINT_INTERVAL == 0 and len(ground_truth_labels) > 0:  # Every CHECKPOINT_INTERVAL\n",
    "        checkpoint_data = {  # Create checkpoint data\n",
    "            \"labels\": ground_truth_labels,  # Labels list\n",
    "            \"processed_indices\": list(processed_indices),  # Processed indices\n",
    "        }\n",
    "        with open(checkpoint_path, 'w') as f:  # Open file\n",
    "            json.dump(checkpoint_data, f, indent=2)  # Save checkpoint\n",
    "        elapsed = (datetime.now() - start_time).total_seconds() / 60  # Calculate elapsed time\n",
    "        print(f\"\\n   üíæ Checkpoint: {len(ground_truth_labels)} samples ({elapsed:.1f} min)\")  # Confirm\n",
    "        print(f\"      Train: {stats['by_split']['train']}, Val: {stats['by_split']['validation']}, Test: {stats['by_split']['test']}\")  # Split counts\n",
    "\n",
    "# Final statistics\n",
    "end_time = datetime.now()  # Record end time\n",
    "total_duration = (end_time - start_time).total_seconds() / 60  # Calculate total duration\n",
    "\n",
    "print(f\"\\n{'='*60}\")  # Separator\n",
    "print(\"GENERATION COMPLETE\")  # Header\n",
    "print(f\"{'='*60}\")  # Separator\n",
    "print(f\"\\nüìä Final Statistics:\")  # Stats header\n",
    "print(f\"   Total Attempted: {stats['total_attempted']}\")  # Total attempts\n",
    "print(f\"   Successful: {stats['successful']}\")  # Successes\n",
    "print(f\"   Failed: {stats['failed']}\")  # Failures\n",
    "print(f\"   Success Rate: {100*stats['successful']/max(1,stats['total_attempted']):.1f}%\")  # Success rate\n",
    "print(f\"\\nüìä By Split:\")  # Split stats\n",
    "print(f\"   Train: {stats['by_split']['train']}\")  # Train count\n",
    "print(f\"   Validation: {stats['by_split']['validation']}\")  # Val count\n",
    "print(f\"   Test: {stats['by_split']['test']}\")  # Test count\n",
    "print(f\"\\n‚è±Ô∏è Duration: {total_duration:.1f} minutes\")  # Duration\n",
    "if stats['total_attempted'] > 0:  # If any attempts\n",
    "    print(f\"   Avg: {total_duration*60/stats['total_attempted']:.2f} sec/sample\")  # Avg time\n",
    "\n",
    "if stats[\"errors\"]:  # If errors\n",
    "    print(f\"\\n‚ö†Ô∏è Errors ({len(stats['errors'])}):\")  # Error header\n",
    "    for err in stats[\"errors\"][:5]:  # Show first 5\n",
    "        print(f\"   - {err}\")  # Print error\n",
    "\n",
    "# Save final checkpoint\n",
    "checkpoint_data = {\"labels\": ground_truth_labels, \"processed_indices\": list(processed_indices)}  # Final data\n",
    "with open(checkpoint_path, 'w') as f:  # Open file\n",
    "    json.dump(checkpoint_data, f, indent=2)  # Save\n",
    "print(f\"\\n‚úì Final checkpoint: {checkpoint_path}\")  # Confirmation\n",
    "print(f\"‚úì Total ground truth labels: {len(ground_truth_labels)}\")  # Total count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Quality Check Samples\n",
    "\n",
    "Display sample pairs and verify simplification quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA VALIDATION REPORT\n",
      "======================================================================\n",
      "\n",
      "[1] MISSING DATA CHECK\n",
      "--------------------------------------------------\n",
      "Missing source:     0\n",
      "Empty source:       0\n",
      "Missing simplified: 0\n",
      "Empty simplified:   0\n",
      "\n",
      "[2] DUPLICATE CHECK\n",
      "--------------------------------------------------\n",
      "Total samples:        10020\n",
      "Unique sources:       9999\n",
      "Duplicate sources:    21\n",
      "Extra rows (dupes):   21\n",
      "\n",
      "[3] CLEANING DATA\n",
      "--------------------------------------------------\n",
      "Removed (empty):      0\n",
      "Removed (duplicates): 21\n",
      "Clean samples:        9999\n",
      "\n",
      "[4] TOKEN STATISTICS\n",
      "--------------------------------------------------\n",
      "Total input tokens:  6,886,833\n",
      "Total output tokens: 4,558,675\n",
      "Avg input/sample:    689\n",
      "Avg output/sample:   456\n",
      "\n",
      "[5] SAVE CLEANED DATA\n",
      "--------------------------------------------------\n",
      "‚úì Saved to: medisimplifier/checkpoints/ground_truth_clean.json\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Original:  10020\n",
      "Clean:     9999\n",
      "Removed:   21\n",
      "Status:    ‚ö† CHECK DATA\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Cell: Validate Ground Truth Data \n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Load the ground truth checkpoint\n",
    "CHECKPOINT_PATH = \"medisimplifier/checkpoints/ground_truth_checkpoint.json\"\n",
    "\n",
    "with open(CHECKPOINT_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "all_samples = data['labels']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use correct key names\n",
    "SOURCE_KEY = 'complex_text'\n",
    "SIMPLIFIED_KEY = 'simplified_text'\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Missing/Empty check\n",
    "# =============================================================================\n",
    "print(\"\\n[1] MISSING DATA CHECK\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "missing_source = sum(1 for s in all_samples if s.get(SOURCE_KEY) is None)\n",
    "missing_simplified = sum(1 for s in all_samples if s.get(SIMPLIFIED_KEY) is None)\n",
    "empty_source = sum(1 for s in all_samples if s.get(SOURCE_KEY) and len(s[SOURCE_KEY].strip()) == 0)\n",
    "empty_simplified = sum(1 for s in all_samples if s.get(SIMPLIFIED_KEY) and len(s[SIMPLIFIED_KEY].strip()) == 0)\n",
    "\n",
    "print(f\"Missing source:     {missing_source}\")\n",
    "print(f\"Empty source:       {empty_source}\")\n",
    "print(f\"Missing simplified: {missing_simplified}\")\n",
    "print(f\"Empty simplified:   {empty_simplified}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Duplicate check\n",
    "# =============================================================================\n",
    "print(\"\\n[2] DUPLICATE CHECK\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "source_texts = [s[SOURCE_KEY] for s in all_samples]\n",
    "source_counts = Counter(source_texts)\n",
    "duplicate_sources = {k: v for k, v in source_counts.items() if v > 1}\n",
    "\n",
    "print(f\"Total samples:        {len(all_samples)}\")\n",
    "print(f\"Unique sources:       {len(source_counts)}\")\n",
    "print(f\"Duplicate sources:    {len(duplicate_sources)}\")\n",
    "print(f\"Extra rows (dupes):   {sum(duplicate_sources.values()) - len(duplicate_sources)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Clean data\n",
    "# =============================================================================\n",
    "print(\"\\n[3] CLEANING DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "seen_sources = set()\n",
    "clean_samples = []\n",
    "removed_duplicates = 0\n",
    "removed_empty = 0\n",
    "\n",
    "for sample in all_samples:\n",
    "    source = sample.get(SOURCE_KEY, '')\n",
    "    simplified = sample.get(SIMPLIFIED_KEY, '')\n",
    "    \n",
    "    if not source.strip() or not simplified.strip():\n",
    "        removed_empty += 1\n",
    "        continue\n",
    "    \n",
    "    if source in seen_sources:\n",
    "        removed_duplicates += 1\n",
    "        continue\n",
    "    \n",
    "    seen_sources.add(source)\n",
    "    clean_samples.append(sample)\n",
    "\n",
    "print(f\"Removed (empty):      {removed_empty}\")\n",
    "print(f\"Removed (duplicates): {removed_duplicates}\")\n",
    "print(f\"Clean samples:        {len(clean_samples)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Token stats\n",
    "# =============================================================================\n",
    "print(\"\\n[4] TOKEN STATISTICS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "total_input = sum(s.get('input_tokens', 0) for s in clean_samples)\n",
    "total_output = sum(s.get('output_tokens', 0) for s in clean_samples)\n",
    "\n",
    "print(f\"Total input tokens:  {total_input:,}\")\n",
    "print(f\"Total output tokens: {total_output:,}\")\n",
    "print(f\"Avg input/sample:    {total_input / len(clean_samples):,.0f}\")\n",
    "print(f\"Avg output/sample:   {total_output / len(clean_samples):,.0f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Save cleaned data\n",
    "# =============================================================================\n",
    "print(\"\\n[5] SAVE CLEANED DATA\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "CLEAN_PATH = \"medisimplifier/checkpoints/ground_truth_clean.json\"\n",
    "\n",
    "clean_data = {\n",
    "    'labels': clean_samples,\n",
    "    'total_input_tokens': total_input,\n",
    "    'total_output_tokens': total_output\n",
    "}\n",
    "\n",
    "with open(CLEAN_PATH, \"w\") as f:\n",
    "    json.dump(clean_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Saved to: {CLEAN_PATH}\")\n",
    "\n",
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original:  {len(all_samples)}\")\n",
    "print(f\"Clean:     {len(clean_samples)}\")\n",
    "print(f\"Removed:   {removed_empty + removed_duplicates}\")\n",
    "print(f\"Status:    {'‚úì READY' if len(clean_samples) >= 10000 else '‚ö† CHECK DATA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 9999 clean samples\n",
      "============================================================\n",
      "QUALITY CHECK OF GROUND TRUE LABLED DATA: SAMPLE PAIRS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SAMPLE 1 (Index: 1824)\n",
      "============================================================\n",
      "\n",
      "üìÑ ORIGINAL (Flesch-Kincaid: 16.5):\n",
      "----------------------------------------\n",
      "Discharge Summary\n",
      "\n",
      "Patient Name: N/A\n",
      "Medical Record Number: N/A\n",
      "Admission Date: N/A\n",
      "Discharge Date: N/A\n",
      "\n",
      "Hospital Course Summary:\n",
      "\n",
      "A 36-year-old Japanese man with a 14-month history of HIV infection was admitted to the hospital with various symptoms, including fever, pulsating headache, lumbago, nausea, and vomiting. Although he was receiving highly active anti-retroviral therapy (HAART) consisting of lamivudine, azidothymidine, and lopinavir plus ritonavir for pneumocystis pneumonia, his CD4-po...\n",
      "\n",
      "üìù SIMPLIFIED (Flesch-Kincaid: 9.3):\n",
      "----------------------------------------\n",
      "Discharge Summary\n",
      "\n",
      "Patient Name: N/A\n",
      "Medical Record Number: N/A\n",
      "Admission Date: N/A\n",
      "Discharge Date: N/A\n",
      "\n",
      "Hospital Course Summary:\n",
      "\n",
      "A 36-year-old Japanese man with HIV infection for 14 months was admitted to the hospital. He had fever, a throbbing headache, lower back pain, nausea, and vomiting. He was taking a mix of HIV medicines called lamivudine, azidothymidine, and lopinavir plus ritonavir. These medicines were given for a lung infection caused by a fungus. However, his immune cell count in ...\n",
      "\n",
      "üìä Readability Improvement: +7.2 grade levels\n",
      "\n",
      "============================================================\n",
      "SAMPLE 2 (Index: 409)\n",
      "============================================================\n",
      "\n",
      "üìÑ ORIGINAL (Flesch-Kincaid: 14.4):\n",
      "----------------------------------------\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [redacted]\n",
      "Age: 15\n",
      "Gender: Male\n",
      "Date of Admission: [redacted]\n",
      "Date of Discharge: [redacted]\n",
      "Admitting Diagnosis: Transverse fracture of the left little finger proximal phalanx with ulnar displacement of the phalangeal head and an undisplaced fracture of the basal metaphysis of the middle phalanx\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "The patient was admitted to the emergency department with a history of hyperextension injury to the left little finger while playing football. Radiogr...\n",
      "\n",
      "üìù SIMPLIFIED (Flesch-Kincaid: 6.5):\n",
      "----------------------------------------\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [redacted]\n",
      "Age: 15\n",
      "Gender: Male\n",
      "Date of Admission: [redacted]\n",
      "Date of Discharge: [redacted]\n",
      "Admitting Diagnosis: A broken bone going straight across the first bone of the left pinky finger, with the bone end pushed toward the outer side of the hand, and a small crack at the base of the second bone of the same finger\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "The patient came to the emergency room after bending the left pinky finger too far back while playing football. Hand x-rays showe...\n",
      "\n",
      "üìä Readability Improvement: +7.9 grade levels\n",
      "\n",
      "============================================================\n",
      "SAMPLE 3 (Index: 4506)\n",
      "============================================================\n",
      "\n",
      "üìÑ ORIGINAL (Flesch-Kincaid: 17.3):\n",
      "----------------------------------------\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [redacted]\n",
      "Medical Record Number: [redacted]\n",
      "Date of Admission: [redacted]\n",
      "Date of Discharge: [redacted]\n",
      "Hospital Name: [redacted]\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "The patient is a 23-year-old Hispanic female who presented with chest pain, dyspnoea, and fatigue. Initial investigations indicated that she may have pericarditis; however, this was ruled out after further evaluation.\n",
      "\n",
      "Subsequent clinical findings indicated a possible cardiac cause, and an electrocardiogram (ECG) r...\n",
      "\n",
      "üìù SIMPLIFIED (Flesch-Kincaid: 6.6):\n",
      "----------------------------------------\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [redacted]\n",
      "Medical Record Number: [redacted]\n",
      "Date of Admission: [redacted]\n",
      "Date of Discharge: [redacted]\n",
      "Hospital Name: [redacted]\n",
      "\n",
      "Hospital Course:\n",
      "\n",
      "The patient is a 23-year-old Hispanic female who came to the hospital with chest pain, trouble breathing, and feeling very tired. At first, doctors thought she might have swelling around her heart. More tests showed this was not the case.\n",
      "\n",
      "More tests pointed to a heart problem. A heart tracing test showed signs tha...\n",
      "\n",
      "üìä Readability Improvement: +10.7 grade levels\n",
      "\n",
      "============================================================\n",
      "OVERALL QUALITY STATISTICS OF LABLED DATA\n",
      "============================================================\n",
      "\n",
      "üìä Average Original Flesch-Kincaid: 14.5\n",
      "üìä Average Simplified Flesch-Kincaid: 7.2\n",
      "üìä Average Improvement: 7.3 grade levels\n",
      "üìå Target Reading Level: Grade 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Quality Check of Ground True Labled Data: Display Sample Pairs\n",
    "# ============================================================================\n",
    "\n",
    "import json  # Import json module\n",
    "import random  # Import random module\n",
    "import textstat  # Import textstat for readability metrics\n",
    "import numpy as np  # Import numpy for calculations\n",
    "\n",
    "# Define readability calculation function\n",
    "def calculate_readability(text: str) -> dict:\n",
    "    \"\"\"Calculate Flesch-Kincaid Grade Level and Flesch Reading Ease.\"\"\"\n",
    "    if not text or len(text.split()) < 3:  # Check if text is too short\n",
    "        return {  # Return default values for invalid text\n",
    "            \"flesch_kincaid_grade\": 0.0,  # Default grade level\n",
    "            \"flesch_reading_ease\": 0.0,  # Default reading ease\n",
    "        }\n",
    "    return {  # Return readability scores\n",
    "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),  # Grade level (lower is simpler)\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),  # Reading ease (higher is simpler)\n",
    "    }\n",
    "\n",
    "# Load the clean ground truth data\n",
    "with open(\"medisimplifier/checkpoints/ground_truth_clean.json\", \"r\") as f:  # Open file\n",
    "    data = json.load(f)  # Load JSON data\n",
    "    ground_truth_labels = data[\"labels\"]  # Extract labels list\n",
    "\n",
    "print(f\"‚úì Loaded {len(ground_truth_labels)} clean samples\")  # Confirmation\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"QUALITY CHECK OF GROUND TRUE LABLED DATA: SAMPLE PAIRS\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "# Number of samples to display\n",
    "NUM_SAMPLES_TO_DISPLAY = 3  # Show 3 random samples\n",
    "\n",
    "# Select random samples\n",
    "sample_indices = random.sample(range(len(ground_truth_labels)), NUM_SAMPLES_TO_DISPLAY)  # Random selection\n",
    "\n",
    "# Display each sample pair\n",
    "for i, sample_idx in enumerate(sample_indices, 1):  # Iterate through samples\n",
    "    sample = ground_truth_labels[sample_idx]  # Get sample\n",
    "    \n",
    "    # Calculate readability metrics\n",
    "    original_readability = calculate_readability(sample[\"complex_text\"])  # Original readability\n",
    "    simplified_readability = calculate_readability(sample[\"simplified_text\"])  # Simplified readability\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")  # Print separator\n",
    "    print(f\"SAMPLE {i} (Index: {sample_idx})\")  # Print sample header\n",
    "    print(f\"{'='*60}\")  # Print separator\n",
    "    \n",
    "    print(f\"\\nüìÑ ORIGINAL (Flesch-Kincaid: {original_readability['flesch_kincaid_grade']:.1f}):\")  # Original header\n",
    "    print(f\"-\" * 40)  # Separator\n",
    "    original_text = sample[\"complex_text\"]  # Get original text\n",
    "    print(original_text[:500] + \"...\" if len(original_text) > 500 else original_text)  # Truncated original\n",
    "    \n",
    "    print(f\"\\nüìù SIMPLIFIED (Flesch-Kincaid: {simplified_readability['flesch_kincaid_grade']:.1f}):\")  # Simplified header\n",
    "    print(f\"-\" * 40)  # Separator\n",
    "    simplified_text = sample[\"simplified_text\"]  # Get simplified text\n",
    "    print(simplified_text[:500] + \"...\" if len(simplified_text) > 500 else simplified_text)  # Truncated simplified\n",
    "    \n",
    "    # Calculate improvement\n",
    "    grade_improvement = original_readability['flesch_kincaid_grade'] - simplified_readability['flesch_kincaid_grade']  # Grade improvement\n",
    "    print(f\"\\nüìä Readability Improvement: {grade_improvement:+.1f} grade levels\")  # Print improvement\n",
    "\n",
    "# Calculate overall statistics\n",
    "print(f\"\\n{'='*60}\")  # Print separator\n",
    "print(\"OVERALL QUALITY STATISTICS OF LABLED DATA\")  # Print header\n",
    "print(f\"{'='*60}\")  # Print separator\n",
    "\n",
    "# Calculate average readability for all samples (sample up to 100)\n",
    "all_original_grades = []  # List for original grades\n",
    "all_simplified_grades = []  # List for simplified grades\n",
    "\n",
    "for sample in ground_truth_labels[:min(10000, len(ground_truth_labels))]:  # Scan entire dataset to claculate averages \n",
    "    orig_read = calculate_readability(sample[\"complex_text\"])  # Original readability\n",
    "    simp_read = calculate_readability(sample[\"simplified_text\"])  # Simplified readability\n",
    "    all_original_grades.append(orig_read[\"flesch_kincaid_grade\"])  # Add original grade\n",
    "    all_simplified_grades.append(simp_read[\"flesch_kincaid_grade\"])  # Add simplified grade\n",
    "\n",
    "print(f\"\\nüìä Average Original Flesch-Kincaid: {np.mean(all_original_grades):.1f}\")  # Mean original\n",
    "print(f\"üìä Average Simplified Flesch-Kincaid: {np.mean(all_simplified_grades):.1f}\")  # Mean simplified\n",
    "print(f\"üìä Average Improvement: {np.mean(all_original_grades) - np.mean(all_simplified_grades):.1f} grade levels\")  # Mean improvement\n",
    "print(f\"üìå Target Reading Level: Grade {TARGET_READING_LEVEL}\")  # Show target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Split into Train/Validation/Test\n",
    "\n",
    "Split the 9,999 labeled pairs (complex + simplified) into training, validation, and test sets.\n",
    "\n",
    "| Split | Percentage | Purpose |\n",
    "|-------|------------|---------|\n",
    "| **Train** | 80% | LoRA fine-tuning |\n",
    "| **Validation** | 10% | Hyperparameter tuning, early stopping |\n",
    "| **Test** | 10% | Final evaluation (baseline + fine-tuned) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPLITTING LABELED DATA INTO TRAIN/VAL/TEST\n",
      "============================================================\n",
      "\n",
      "üìÇ Loading clean ground truth data...\n",
      "   ‚úì Loaded 9999 labeled pairs\n",
      "\n",
      "üìä Split Configuration:\n",
      "   Total samples: 9,999\n",
      "   Train: 7,999 (80%)\n",
      "   Validation: 999 (10%)\n",
      "   Test: 1,001 (10%)\n",
      "\n",
      "üîÄ Shuffling with seed=42...\n",
      "‚úÇÔ∏è  Splitting data...\n",
      "\n",
      "‚úì Splits created successfully!\n",
      "\n",
      "üìä Final Split Sizes:\n",
      "   train: 7,999 samples\n",
      "   validation: 999 samples\n",
      "   test: 1,001 samples\n",
      "\n",
      "   Total: 9,999 samples\n",
      "\n",
      "üíæ Saving splits to disk...\n",
      "   ‚úì Saved to: medisimplifier/data/labeled_splits.json\n",
      "\n",
      "üìã Sample from each split:\n",
      "----------------------------------------\n",
      "\n",
      "TRAIN:\n",
      "   Complex: Discharge Summary:\n",
      "\n",
      "Patient Information:\n",
      "Name: [Patient's Name]\n",
      "DOB: [Patient's Date of Birth]\n",
      "Gende...\n",
      "   Simplified: Discharge Summary:\n",
      "\n",
      "Patient Information:\n",
      "Name: [Patient's Name]\n",
      "DOB: [Patient's Date of Birth]\n",
      "Gende...\n",
      "\n",
      "VALIDATION:\n",
      "   Complex: Discharge Summary:\n",
      "\n",
      "Patient Name: [Name]\n",
      "Sex: Male\n",
      "Age: 47 days\n",
      "Admission Date: [Date]\n",
      "Discharge Dat...\n",
      "   Simplified: Discharge Summary:\n",
      "\n",
      "Patient Name: [Name]\n",
      "Sex: Male\n",
      "Age: 47 days\n",
      "Admission Date: [Date]\n",
      "Discharge Dat...\n",
      "\n",
      "TEST:\n",
      "   Complex: Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The...\n",
      "   Simplified: Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The...\n",
      "\n",
      "============================================================\n",
      "‚úì Data splitting complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Split Ground Truth Data into Train/Validation/Test\n",
    "# ============================================================================\n",
    "\n",
    "import json  # Import json module\n",
    "import random  # Import random module\n",
    "\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "print(\"SPLITTING LABELED DATA INTO TRAIN/VAL/TEST\")  # Print section header\n",
    "print(\"=\" * 60)  # Print separator line\n",
    "\n",
    "# Load the clean ground truth data\n",
    "print(\"\\nüìÇ Loading clean ground truth data...\")  # Status message\n",
    "with open(\"medisimplifier/checkpoints/ground_truth_clean.json\", \"r\") as f:  # Open file\n",
    "    data = json.load(f)  # Load JSON data\n",
    "    ground_truth_labels = data[\"labels\"]  # Extract labels list\n",
    "\n",
    "print(f\"   ‚úì Loaded {len(ground_truth_labels)} labeled pairs\")  # Confirmation\n",
    "\n",
    "# ============================================================================\n",
    "# Define Split Ratios\n",
    "# ============================================================================\n",
    "\n",
    "SEED = 42  # Random seed for reproducibility\n",
    "TRAIN_RATIO = 0.80  # 80% for training\n",
    "VAL_RATIO = 0.10  # 10% for validation\n",
    "TEST_RATIO = 0.10  # 10% for testing\n",
    "\n",
    "# Calculate split sizes\n",
    "total_samples = len(ground_truth_labels)  # Total number of samples\n",
    "train_size = int(total_samples * TRAIN_RATIO)  # Training set size\n",
    "val_size = int(total_samples * VAL_RATIO)  # Validation set size\n",
    "test_size = total_samples - train_size - val_size  # Test set size (remainder)\n",
    "\n",
    "print(f\"\\nüìä Split Configuration:\")  # Section header\n",
    "print(f\"   Total samples: {total_samples:,}\")  # Display total\n",
    "print(f\"   Train: {train_size:,} ({TRAIN_RATIO*100:.0f}%)\")  # Display train size\n",
    "print(f\"   Validation: {val_size:,} ({VAL_RATIO*100:.0f}%)\")  # Display val size\n",
    "print(f\"   Test: {test_size:,} ({TEST_RATIO*100:.0f}%)\")  # Display test size\n",
    "\n",
    "# ============================================================================\n",
    "# Shuffle and Split\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüîÄ Shuffling with seed={SEED}...\")  # Status message\n",
    "random.seed(SEED)  # Set random seed for reproducibility\n",
    "shuffled_data = ground_truth_labels.copy()  # Create a copy to shuffle\n",
    "random.shuffle(shuffled_data)  # Shuffle in place\n",
    "\n",
    "# Split the data\n",
    "print(f\"‚úÇÔ∏è  Splitting data...\")  # Status message\n",
    "train_data = shuffled_data[:train_size]  # First portion for training\n",
    "val_data = shuffled_data[train_size:train_size + val_size]  # Middle portion for validation\n",
    "test_data = shuffled_data[train_size + val_size:]  # Remaining for test\n",
    "\n",
    "# Create splits dictionary\n",
    "dataset_splits = {  # Dictionary containing all splits\n",
    "    \"train\": train_data,  # Training set\n",
    "    \"validation\": val_data,  # Validation set\n",
    "    \"test\": test_data  # Test set\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Verify Splits\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n‚úì Splits created successfully!\")  # Success message\n",
    "print(f\"\\nüìä Final Split Sizes:\")  # Section header\n",
    "for split_name, split_data in dataset_splits.items():  # Iterate through splits\n",
    "    print(f\"   {split_name}: {len(split_data):,} samples\")  # Display split size\n",
    "\n",
    "# Verify total matches\n",
    "total_in_splits = sum(len(split) for split in dataset_splits.values())  # Sum all splits\n",
    "print(f\"\\n   Total: {total_in_splits:,} samples\")  # Display total\n",
    "assert total_in_splits == total_samples, \"Split sizes don't match total!\"  # Verify\n",
    "\n",
    "# ============================================================================\n",
    "# Save Splits to Disk\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüíæ Saving splits to disk...\")  # Status message\n",
    "\n",
    "# Define save path\n",
    "splits_path = Path(\"medisimplifier/data/labeled_splits.json\")  # Path for splits file\n",
    "\n",
    "# Save as JSON\n",
    "splits_to_save = {  # Data structure to save\n",
    "    \"train\": train_data,  # Training data\n",
    "    \"validation\": val_data,  # Validation data\n",
    "    \"test\": test_data,  # Test data\n",
    "    \"metadata\": {  # Metadata about the splits\n",
    "        \"total_samples\": total_samples,  # Total count\n",
    "        \"train_size\": train_size,  # Train count\n",
    "        \"val_size\": val_size,  # Val count\n",
    "        \"test_size\": test_size,  # Test count\n",
    "        \"seed\": SEED,  # Random seed used\n",
    "        \"source\": \"ground_truth_clean.json\"  # Source file\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(splits_path, \"w\") as f:  # Open file for writing\n",
    "    json.dump(splits_to_save, f, indent=2)  # Save with formatting\n",
    "\n",
    "print(f\"   ‚úì Saved to: {splits_path}\")  # Confirmation\n",
    "\n",
    "# ============================================================================\n",
    "# Display Sample from Each Split\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìã Sample from each split:\")  # Section header\n",
    "print(\"-\" * 40)  # Separator\n",
    "\n",
    "for split_name, split_data in dataset_splits.items():  # Iterate through splits\n",
    "    sample = split_data[0]  # Get first sample\n",
    "    complex_preview = sample[\"complex_text\"][:100] + \"...\"  # Truncate complex text\n",
    "    simplified_preview = sample[\"simplified_text\"][:100] + \"...\"  # Truncate simplified text\n",
    "    print(f\"\\n{split_name.upper()}:\")  # Split name\n",
    "    print(f\"   Complex: {complex_preview}\")  # Show complex preview\n",
    "    print(f\"   Simplified: {simplified_preview}\")  # Show simplified preview\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)  # Print separator line\n",
    "print(\"‚úì Data splitting complete!\")  # Final confirmation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
