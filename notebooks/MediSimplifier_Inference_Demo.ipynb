{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediSimplifier - Inference Demonstration\n",
    "\n",
    "**Medical Discharge Summary Simplification using LoRA Fine-Tuned Models**\n",
    "\n",
    "This notebook demonstrates inference with all three MediSimplifier models hosted on HuggingFace:\n",
    "- **OpenBioLLM-8B** üèÜ (Best overall performance)\n",
    "- **Mistral-7B** (Best readability)\n",
    "- **BioMistral-7B-DARE** (Medical baseline)\n",
    "\n",
    "**Authors:** Guy Dor & Shmulik Avraham  \n",
    "**Course:** DS25 Deep Learning, Technion  \n",
    "**Date:** January 2026\n",
    "\n",
    "### Resources\n",
    "- **Models:** [HuggingFace](https://huggingface.co/GuyDor007/MediSimplifier-LoRA-Adapters)\n",
    "- **Dataset:** [HuggingFace](https://huggingface.co/datasets/GuyDor007/medisimplifier-dataset)\n",
    "- **Code:** [GitHub](https://github.com/gd007/MediSimplifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch transformers peft datasets accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    DTYPE = torch.bfloat16\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    DTYPE = torch.float32  # MPS works better with float32\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DTYPE = torch.float32\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MediSimplifier dataset\n",
    "dataset = load_dataset(\"GuyDor007/medisimplifier-dataset\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"  Train:      {len(dataset['train']):,} samples\")\n",
    "print(f\"  Validation: {len(dataset['validation']):,} samples\")\n",
    "print(f\"  Test:       {len(dataset['test']):,} samples\")\n",
    "print(f\"  Columns:    {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test sample for demonstration\n",
    "TEST_INDEX = 0  # Change this to test different samples\n",
    "test_sample = dataset['test'][TEST_INDEX]\n",
    "\n",
    "medical_text = test_sample['input']\n",
    "ground_truth = test_sample['output']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ORIGINAL MEDICAL TEXT\")\n",
    "print(\"=\" * 70)\n",
    "print(medical_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "All models are hosted in a single HuggingFace repo with subfolders:\n",
    "- `openbiollm_8b_lora/` - Llama3 architecture, uses **ChatML** format\n",
    "- `mistral_7b_lora/` - Mistral architecture, uses **Mistral** format\n",
    "- `biomistral_7b_dare_lora/` - Mistral architecture, uses **Mistral** format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace repo containing all adapters\n",
    "HF_REPO = \"GuyDor007/MediSimplifier-LoRA-Adapters\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"OpenBioLLM-8B\": {\n",
    "        \"base_model\": \"aaditya/Llama3-OpenBioLLM-8B\",\n",
    "        \"adapter_subfolder\": \"openbiollm_8b_lora\",\n",
    "        \"format\": \"chatml\",\n",
    "    },\n",
    "    \"Mistral-7B\": {\n",
    "        \"base_model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"adapter_subfolder\": \"mistral_7b_lora\",\n",
    "        \"format\": \"mistral\",\n",
    "    },\n",
    "    \"BioMistral-7B\": {\n",
    "        \"base_model\": \"BioMistral/BioMistral-7B-DARE\",\n",
    "        \"adapter_subfolder\": \"biomistral_7b_dare_lora\",\n",
    "        \"format\": \"mistral\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Model configurations loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates & Utility Functions\n",
    "\n",
    "**Important:** Each model uses its native prompt format:\n",
    "- **OpenBioLLM-8B:** ChatML format (`<|im_start|>...<|im_end|>`)\n",
    "- **Mistral-7B / BioMistral-7B:** Mistral format (`[INST]...[/INST]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message and task instruction (consistent across all models)\n",
    "SYSTEM_MESSAGE = \"You are a helpful medical assistant that simplifies complex medical text for patients.\"\n",
    "\n",
    "TASK_INSTRUCTION = \"\"\"Simplify the following medical discharge summary in plain language for patients with no medical background.\n",
    "Guidelines:\n",
    "- Replace medical jargon with everyday words (e.g., \"hypertension\" ‚Üí \"high blood pressure\")\n",
    "- Keep all important information (diagnoses, medications, follow-up instructions)\n",
    "- Use short, clear sentences (aim for 15-20 words per sentence)\n",
    "- Aim for a 6th-grade reading level\n",
    "- Maintain the same structure as the original\n",
    "- Do not add or omit information\n",
    "- Keep the same patient reference style (e.g., \"The patient\" stays \"The patient\", not \"You\")\n",
    "- Output plain text only (no markdown, no bold, no headers, no bullet points)\n",
    "- Do not include empty lines or separator characters like \"---\"\"\"\"\n",
    "\n",
    "\n",
    "def build_prompt(medical_text: str, format_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt using the correct format for each model architecture.\n",
    "    \n",
    "    Args:\n",
    "        medical_text: The medical discharge summary to simplify\n",
    "        format_type: 'chatml' for OpenBioLLM, 'mistral' for Mistral/BioMistral\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    if format_type == \"chatml\":\n",
    "        # ChatML format for OpenBioLLM-8B (Llama3 architecture)\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_MESSAGE}<|im_end|>\n",
    "<|im_start|>user\n",
    "{TASK_INSTRUCTION}\n",
    "\n",
    "{medical_text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    elif format_type == \"mistral\":\n",
    "        # Mistral format for Mistral-7B and BioMistral-7B\n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "{SYSTEM_MESSAGE}\n",
    "<</SYS>>\n",
    "\n",
    "{TASK_INSTRUCTION}\n",
    "\n",
    "{medical_text} [/INST]\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format type: {format_type}\")\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def clean_output(generated_text: str, format_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Post-process model output to clean up any artifacts.\n",
    "    \n",
    "    Args:\n",
    "        generated_text: Raw generated tokens (after token slicing)\n",
    "        format_type: The prompt format used\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned simplified text\n",
    "    \"\"\"\n",
    "    cleaned = generated_text.strip()\n",
    "    \n",
    "    # Remove format-specific tokens\n",
    "    if format_type == \"chatml\":\n",
    "        for token in [\"<|im_start|>\", \"<|im_end|>\", \"<|end_of_text|>\", \"<|eot_id|>\"]:\n",
    "            cleaned = cleaned.replace(token, \"\").strip()\n",
    "        # Truncate if model starts new turn\n",
    "        if \"<|im_start|>user\" in cleaned:\n",
    "            cleaned = cleaned.split(\"<|im_start|>user\")[0].strip()\n",
    "    \n",
    "    elif format_type == \"mistral\":\n",
    "        for token in [\"</s>\", \"<s>\", \"[INST]\", \"[/INST]\"]:\n",
    "            cleaned = cleaned.replace(token, \"\").strip()\n",
    "        # Truncate if model starts new instruction\n",
    "        if \"[INST]\" in cleaned:\n",
    "            cleaned = cleaned.split(\"[INST]\")[0].strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Load base model with LoRA adapter from HuggingFace.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Key from MODELS dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer, format_type)\n",
    "    \"\"\"\n",
    "    config = MODELS[model_name]\n",
    "    \n",
    "    print(f\"  Loading base model: {config['base_model']}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config['base_model'],\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if DEVICE == \"mps\":\n",
    "        base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    print(f\"  Loading LoRA adapter: {HF_REPO}/{config['adapter_subfolder']}\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        HF_REPO,\n",
    "        subfolder=config['adapter_subfolder']\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"  Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config['base_model'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer, config['format']\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, medical_text: str, format_type: str, max_new_tokens: int = 768) -> str:\n",
    "    \"\"\"\n",
    "    Run inference on a single medical text.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded model with LoRA adapter\n",
    "        tokenizer: The tokenizer\n",
    "        medical_text: Input medical text to simplify\n",
    "        format_type: 'chatml' or 'mistral'\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Simplified text\n",
    "    \"\"\"\n",
    "    # Build prompt with correct format\n",
    "    prompt = build_prompt(medical_text, format_type)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated portion\n",
    "    generated_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=False)\n",
    "    \n",
    "    # Clean output\n",
    "    simplified = clean_output(generated_text, format_type)\n",
    "    \n",
    "    return simplified\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")\n",
    "print(\"  - OpenBioLLM-8B uses ChatML format\")\n",
    "print(\"  - Mistral-7B and BioMistral-7B use Mistral format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference with OpenBioLLM-8B (Best Model) üèÜ\n",
    "\n",
    "**Best overall performance:**\n",
    "- ROUGE-L: **0.6749**\n",
    "- SARI: **74.64**\n",
    "- BERTScore: **0.9498**\n",
    "- FK-Grade: 7.16\n",
    "- Improvement: **+157.3%** over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading OpenBioLLM-8B with LoRA adapter...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_openbio, tokenizer_openbio, format_openbio = load_model(\"OpenBioLLM-8B\")\n",
    "\n",
    "print(\"\\n‚úÖ OpenBioLLM-8B loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating simplified text with OpenBioLLM-8B...\")\n",
    "print(\"(This may take 1-2 minutes on CPU/MPS)\\n\")\n",
    "\n",
    "output_openbio = run_inference(model_openbio, tokenizer_openbio, medical_text, format_openbio)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OpenBioLLM-8B OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(output_openbio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory before loading next model\n",
    "del model_openbio, tokenizer_openbio\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference with Mistral-7B (Best Readability)\n",
    "\n",
    "**Best readability score:**\n",
    "- ROUGE-L: 0.6491\n",
    "- SARI: 73.79\n",
    "- BERTScore: 0.9464\n",
    "- FK-Grade: **6.91** (closest to target ‚â§6)\n",
    "- Improvement: +65.9% over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading Mistral-7B with LoRA adapter...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_mistral, tokenizer_mistral, format_mistral = load_model(\"Mistral-7B\")\n",
    "\n",
    "print(\"\\n‚úÖ Mistral-7B loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating simplified text with Mistral-7B...\")\n",
    "print(\"(This may take 1-2 minutes on CPU/MPS)\\n\")\n",
    "\n",
    "output_mistral = run_inference(model_mistral, tokenizer_mistral, medical_text, format_mistral)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Mistral-7B OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(output_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory before loading next model\n",
    "del model_mistral, tokenizer_mistral\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference with BioMistral-7B (Medical Baseline)\n",
    "\n",
    "**Medical domain baseline:**\n",
    "- ROUGE-L: 0.6318\n",
    "- SARI: 73.01\n",
    "- BERTScore: 0.9439\n",
    "- FK-Grade: 6.95\n",
    "- Improvement: +53.3% over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading BioMistral-7B with LoRA adapter...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_biomistral, tokenizer_biomistral, format_biomistral = load_model(\"BioMistral-7B\")\n",
    "\n",
    "print(\"\\n‚úÖ BioMistral-7B loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating simplified text with BioMistral-7B...\")\n",
    "print(\"(This may take 1-2 minutes on CPU/MPS)\\n\")\n",
    "\n",
    "output_biomistral = run_inference(model_biomistral, tokenizer_biomistral, medical_text, format_biomistral)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BioMistral-7B OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(output_biomistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del model_biomistral, tokenizer_biomistral\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GROUND TRUTH (Claude-generated reference)\")\n",
    "print(\"=\" * 70)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LENGTH COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Source':<25} {'Characters':>12} {'Ratio':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Input':<25} {len(medical_text):>12,} {1.00:>10.2f}\")\n",
    "print(f\"{'Ground Truth':<25} {len(ground_truth):>12,} {len(ground_truth)/len(medical_text):>10.2f}\")\n",
    "print(f\"{'OpenBioLLM-8B':<25} {len(output_openbio):>12,} {len(output_openbio)/len(medical_text):>10.2f}\")\n",
    "print(f\"{'Mistral-7B':<25} {len(output_mistral):>12,} {len(output_mistral)/len(medical_text):>10.2f}\")\n",
    "print(f\"{'BioMistral-7B':<25} {len(output_biomistral):>12,} {len(output_biomistral)/len(medical_text):>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Performance Summary\n",
    "\n",
    "Results from full test set evaluation (1,001 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance metrics from full evaluation\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL TEST SET PERFORMANCE (1,001 samples)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'ROUGE-L':>10} {'SARI':>10} {'BERTScore':>12} {'FK-Grade':>10} {'Œî vs Base':>12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'OpenBioLLM-8B üèÜ':<20} {'0.6749':>10} {'74.64':>10} {'0.9498':>12} {'7.16':>10} {'+157.3%':>12}\")\n",
    "print(f\"{'Mistral-7B':<20} {'0.6491':>10} {'73.79':>10} {'0.9464':>12} {'6.91':>10} {'+65.9%':>12}\")\n",
    "print(f\"{'BioMistral-7B':<20} {'0.6318':>10} {'73.01':>10} {'0.9439':>12} {'6.95':>10} {'+53.3%':>12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Ground Truth FK:':<20} {'7.23':>10}\")\n",
    "print(f\"{'Source FK:':<20} {'14.50':>10}\")\n",
    "print(f\"{'Target FK:':<20} {'‚â§6.0':>10}\")\n",
    "print(\"\\nüìä All models achieve ~50% readability reduction (college ‚Üí 7th grade level)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings\n",
    "\n",
    "### Ranking Reversal\n",
    "The worst zero-shot model (OpenBioLLM) achieved the **best** fine-tuned performance:\n",
    "\n",
    "| Model | Zero-Shot ROUGE-L | Fine-Tuned ROUGE-L | Improvement |\n",
    "|-------|-------------------|--------------------|--------------|\n",
    "| OpenBioLLM-8B | 0.2623 (worst) | **0.6749** (best) | +157% |\n",
    "| Mistral-7B | 0.3912 | 0.6491 | +66% |\n",
    "| BioMistral-7B | 0.4120 (best) | 0.6318 (worst) | +53% |\n",
    "\n",
    "### Statistical Significance\n",
    "- All pairwise ROUGE-L differences are significant (p < 0.001)\n",
    "- Effect size: OpenBioLLM vs BioMistral = medium (Cohen's d = 0.79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resources\n",
    "\n",
    "### HuggingFace\n",
    "- **Models:** https://huggingface.co/GuyDor007/MediSimplifier-LoRA-Adapters\n",
    "- **Dataset:** https://huggingface.co/datasets/GuyDor007/medisimplifier-dataset\n",
    "\n",
    "### GitHub\n",
    "- **Code & Notebooks:** https://github.com/gd007/MediSimplifier\n",
    "\n",
    "### Prompt Formats\n",
    "Models were trained with their **native formats**:\n",
    "\n",
    "**ChatML (OpenBioLLM-8B):**\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "\n",
    "{input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "**Mistral (Mistral-7B, BioMistral-7B):**\n",
    "```\n",
    "[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "\n",
    "{input} [/INST]\n",
    "```\n",
    "\n",
    "### Citation\n",
    "```bibtex\n",
    "@misc{medisimplifier2026,\n",
    "  title={MediSimplifier: Medical Discharge Summary Simplification using LoRA Fine-Tuning},\n",
    "  author={Dor, Guy and Avraham, Shmulik},\n",
    "  year={2026},\n",
    "  institution={Technion - Israel Institute of Technology}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
