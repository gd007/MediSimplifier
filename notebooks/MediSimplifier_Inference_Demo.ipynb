{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MediSimplifier - Inference Demonstration\n",
    "\n",
    "**Medical Discharge Summary Simplification using LoRA Fine-Tuned Models**\n",
    "\n",
    "This notebook demonstrates inference with all three MediSimplifier models hosted on HuggingFace:\n",
    "- **OpenBioLLM-8B** üèÜ (Best overall performance)\n",
    "- **Mistral-7B** (Best readability)\n",
    "- **BioMistral-7B-DARE** (Medical baseline)\n",
    "\n",
    "**Authors:** Guy Dor & Shmulik Avraham  \n",
    "**Course:** DS25 Deep Learning, Technion  \n",
    "**Date:** January 2026\n",
    "\n",
    "### Resources\n",
    "- **Models:** [HuggingFace](https://huggingface.co/GuyDor007/MediSimplifier-LoRA-Adapters)\n",
    "- **Dataset:** [HuggingFace](https://huggingface.co/datasets/GuyDor007/medisimplifier-dataset)\n",
    "- **Code:** [GitHub](https://github.com/gd007/MediSimplifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch transformers peft datasets accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "    DTYPE = torch.bfloat16\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "    DTYPE = torch.float32  # MPS works better with float32\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "    DTYPE = torch.float32\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "  Train:      7,999 samples\n",
      "  Validation: 999 samples\n",
      "  Test:       1,001 samples\n",
      "  Columns:    ['text', 'instruction', 'input', 'output']\n"
     ]
    }
   ],
   "source": [
    "# Load MediSimplifier dataset\n",
    "dataset = load_dataset(\"GuyDor007/medisimplifier-dataset\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"  Train:      {len(dataset['train']):,} samples\")\n",
    "print(f\"  Validation: {len(dataset['validation']):,} samples\")\n",
    "print(f\"  Test:       {len(dataset['test']):,} samples\")\n",
    "print(f\"  Columns:    {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ORIGINAL MEDICAL TEXT\n",
      "======================================================================\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The patient was admitted to our outpatient clinic with widespread skin-colored, dome-shaped lesions over the face and neck, which gradually increased in size and number over a year. The patient disclosed his HIV-positive status, with a CD4 + count of 82 cells/mm3. The patient also reported using a standard antiretroviral therapy regimen for the last five years. The clinical diagnosis was made due to the presence of molluscae bodies in some of the skin lesions, and the patient was confirmed with molluscum contagiosum. No systemic modalities of retinoid or other antiviral medication were used previously. The patient was prescribed multiple topical and oral medications, but the lesions showed no significant improvement.\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Diagnosis: Molluscum contagiosum\n",
      "\n",
      "Hospital Course Summary:\n",
      "The patient was treated with oral isotretinoin 0.5 mg/kg for one month, considering the unavailability of other proven therapies and the widespread location of the disease. Isotretinoin was used because of its property of being a systemic retinoid with intracellular conversion into tretinoin and its effects on cellular proliferation and differentiation. After one month of follow-up, there was objective and subjective improvement in the appearance and count of the lesions. The patient had no adverse effects of oral isotretinoin, except dry lips, which were managed using petrolatum jelly. The remission was maintained even after two months of follow-up.\n",
      "\n",
      "Condition at Discharge:\n",
      "The patient was discharged from the outpatient clinic with improved, almost no lesions on the right half of the face, and a few lesions remained on the left half of the face. The patient was advised to continue the prescribed treatment and follow-up appointments with the clinician.\n",
      "\n",
      "Instructions at Discharge:\n",
      "The patient was advised to follow-up with the clinician for further evaluation. The patient was instructed to maintain good hygiene and avoid sharing personal items, such as towels and clothing, to prevent the spread of the disease. The patient was advised to continue using petrolatum jelly for dry lips. The patient was advised to continue the prescribed oral isotretinoin dosage. \n",
      "\n",
      "Follow-Up Recommendations:\n",
      "The clinician advised the patient to follow-up with them regularly, as per the national guidelines of Nepal, ensuring optimal ART adherence and monitoring. The patient was advised to contact the clinician immediately if any adverse effects or new symptoms develop. \n",
      "\n",
      "Signed,\n",
      "[REDACTED] (Clinician)\n"
     ]
    }
   ],
   "source": [
    "# Select a test sample for demonstration\n",
    "TEST_INDEX = 0  # Change this to test different samples\n",
    "test_sample = dataset['test'][TEST_INDEX]\n",
    "\n",
    "medical_text = test_sample['input']\n",
    "ground_truth = test_sample['output']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ORIGINAL MEDICAL TEXT\")\n",
    "print(\"=\" * 70)\n",
    "print(medical_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "All models are hosted in a single HuggingFace repo with subfolders:\n",
    "- `openbiollm_8b_lora/` - Llama3 architecture, uses **ChatML** format\n",
    "- `mistral_7b_lora/` - Mistral architecture, uses **Mistral** format\n",
    "- `biomistral_7b_dare_lora/` - Mistral architecture, uses **Mistral** format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations loaded.\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace repo containing all adapters\n",
    "HF_REPO = \"GuyDor007/MediSimplifier-LoRA-Adapters\"\n",
    "\n",
    "# Model configurations\n",
    "MODELS = {\n",
    "    \"OpenBioLLM-8B\": {\n",
    "        \"base_model\": \"aaditya/Llama3-OpenBioLLM-8B\",\n",
    "        \"adapter_subfolder\": \"openbiollm_8b_lora\",\n",
    "        \"format\": \"chatml\",\n",
    "    },\n",
    "    \"Mistral-7B\": {\n",
    "        \"base_model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"adapter_subfolder\": \"mistral_7b_lora\",\n",
    "        \"format\": \"mistral\",\n",
    "    },\n",
    "    \"BioMistral-7B\": {\n",
    "        \"base_model\": \"BioMistral/BioMistral-7B-DARE\",\n",
    "        \"adapter_subfolder\": \"biomistral_7b_dare_lora\",\n",
    "        \"format\": \"mistral\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Model configurations loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates & Utility Functions\n",
    "\n",
    "**Important:** Each model uses its native prompt format:\n",
    "- **OpenBioLLM-8B:** ChatML format (`<|im_start|>...<|im_end|>`)\n",
    "- **Mistral-7B / BioMistral-7B:** Mistral format (`[INST]...[/INST]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined.\n",
      "  - OpenBioLLM-8B uses ChatML format\n",
      "  - Mistral-7B and BioMistral-7B use Mistral format\n"
     ]
    }
   ],
   "source": [
    "# System message and task instruction (consistent across all models)\n",
    "SYSTEM_MESSAGE = \"You are a helpful medical assistant that simplifies complex medical text for patients.\"\n",
    "\n",
    "TASK_INSTRUCTION = \"\"\"Simplify the following medical discharge summary in plain language for patients with no medical background.\n",
    "Guidelines:\n",
    "- Replace medical jargon with everyday words (e.g., \"hypertension\" ‚Üí \"high blood pressure\")\n",
    "- Keep all important information (diagnoses, medications, follow-up instructions)\n",
    "- Use short, clear sentences (aim for 15-20 words per sentence)\n",
    "- Aim for a 6th-grade reading level\n",
    "- Maintain the same structure as the original\n",
    "- Do not add or omit information\n",
    "- Keep the same patient reference style (e.g., \"The patient\" stays \"The patient\", not \"You\")\n",
    "- Output plain text only (no markdown, no bold, no headers, no bullet points)\n",
    "- Do not include empty lines or separator characters like \"---\\\" \"\"\"\n",
    "\n",
    "\n",
    "def build_prompt(medical_text: str, format_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Build prompt using the correct format for each model architecture.\n",
    "    \n",
    "    Args:\n",
    "        medical_text: The medical discharge summary to simplify\n",
    "        format_type: 'chatml' for OpenBioLLM, 'mistral' for Mistral/BioMistral\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    if format_type == \"chatml\":\n",
    "        # ChatML format for OpenBioLLM-8B (Llama3 architecture)\n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_MESSAGE}<|im_end|>\n",
    "<|im_start|>user\n",
    "{TASK_INSTRUCTION}\n",
    "\n",
    "{medical_text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    elif format_type == \"mistral\":\n",
    "        # Mistral format for Mistral-7B and BioMistral-7B\n",
    "        prompt = f\"\"\"[INST] <<SYS>>\n",
    "{SYSTEM_MESSAGE}\n",
    "<</SYS>>\n",
    "\n",
    "{TASK_INSTRUCTION}\n",
    "\n",
    "{medical_text} [/INST]\"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown format type: {format_type}\")\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def clean_output(generated_text: str, format_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Post-process model output to clean up any artifacts.\n",
    "    \n",
    "    Args:\n",
    "        generated_text: Raw generated tokens (after token slicing)\n",
    "        format_type: The prompt format used\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned simplified text\n",
    "    \"\"\"\n",
    "    cleaned = generated_text.strip()\n",
    "    \n",
    "    # Remove format-specific tokens\n",
    "    if format_type == \"chatml\":\n",
    "        for token in [\"<|im_start|>\", \"<|im_end|>\", \"<|end_of_text|>\", \"<|eot_id|>\"]:\n",
    "            cleaned = cleaned.replace(token, \"\").strip()\n",
    "        # Truncate if model starts new turn\n",
    "        if \"<|im_start|>user\" in cleaned:\n",
    "            cleaned = cleaned.split(\"<|im_start|>user\")[0].strip()\n",
    "    \n",
    "    elif format_type == \"mistral\":\n",
    "        for token in [\"</s>\", \"<s>\", \"[INST]\", \"[/INST]\"]:\n",
    "            cleaned = cleaned.replace(token, \"\").strip()\n",
    "        # Truncate if model starts new instruction\n",
    "        if \"[INST]\" in cleaned:\n",
    "            cleaned = cleaned.split(\"[INST]\")[0].strip()\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def load_model(model_name: str):\n",
    "    \"\"\"\n",
    "    Load base model with LoRA adapter from HuggingFace.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Key from MODELS dictionary\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, tokenizer, format_type)\n",
    "    \"\"\"\n",
    "    config = MODELS[model_name]\n",
    "    \n",
    "    print(f\"  Loading base model: {config['base_model']}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config['base_model'],\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if DEVICE == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if DEVICE == \"mps\":\n",
    "        base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    print(f\"  Loading LoRA adapter: {HF_REPO}/{config['adapter_subfolder']}\")\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        HF_REPO,\n",
    "        subfolder=config['adapter_subfolder']\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"  Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config['base_model'],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer, config['format']\n",
    "\n",
    "\n",
    "def run_inference(model, tokenizer, medical_text: str, format_type: str, max_new_tokens: int = 768) -> str:\n",
    "    \"\"\"\n",
    "    Run inference on a single medical text.\n",
    "    \n",
    "    Args:\n",
    "        model: The loaded model with LoRA adapter\n",
    "        tokenizer: The tokenizer\n",
    "        medical_text: Input medical text to simplify\n",
    "        format_type: 'chatml' or 'mistral'\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        Simplified text\n",
    "    \"\"\"\n",
    "    # Build prompt with correct format\n",
    "    prompt = build_prompt(medical_text, format_type)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_length = inputs[\"input_ids\"].shape[1]\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated portion\n",
    "    generated_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=False)\n",
    "    \n",
    "    # Clean output\n",
    "    simplified = clean_output(generated_text, format_type)\n",
    "    \n",
    "    return simplified\n",
    "\n",
    "\n",
    "print(\"Utility functions defined.\")\n",
    "print(\"  - OpenBioLLM-8B uses ChatML format\")\n",
    "print(\"  - Mistral-7B and BioMistral-7B use Mistral format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference with OpenBioLLM-8B (Best Model) üèÜ\n",
    "\n",
    "**Best overall performance:**\n",
    "- ROUGE-L: **0.6749**\n",
    "- SARI: **74.64**\n",
    "- BERTScore: **0.9498**\n",
    "- FK-Grade: 7.16\n",
    "- Improvement: **+157.3%** over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading OpenBioLLM-8B with LoRA adapter...\n",
      "======================================================================\n",
      "  Loading base model: aaditya/Llama3-OpenBioLLM-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd66c8c1507342b7ab2f925d7740278d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading LoRA adapter: GuyDor007/MediSimplifier-LoRA-Adapters/openbiollm_8b_lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00c84b8f2994c0b9bde3d1743b59548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3deda8277b5f4f76a349d8602d3ef2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openbiollm_8b_lora/adapter_model.safeten(‚Ä¶):   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d722315799247e89946d2306c90dd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86302a850b104f338ba499e969c3e215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309736b198684b7c9404e76efbaa3e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ OpenBioLLM-8B loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading OpenBioLLM-8B with LoRA adapter...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_openbio, tokenizer_openbio, format_openbio = load_model(\"OpenBioLLM-8B\")\n",
    "\n",
    "print(\"\\n‚úÖ OpenBioLLM-8B loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simplified text with OpenBioLLM-8B...\n",
      "(This may take 1-2 minutes on CPU/MPS)\n",
      "\n",
      "======================================================================\n",
      "OpenBioLLM-8B OUTPUT\n",
      "======================================================================\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The patient came to our clinic with many small, raised bumps on the face and neck. These bumps had slowly grown in number and size over one year. The patient said he has HIV and his immune cell count is 82. The patient has been taking HIV medicine for five years. Doctors found the cause of the bumps by looking at skin samples under a microscope. The patient was diagnosed with a skin infection called molluscum contagiosum. The patient had not tried any other treatments before. The patient was given skin creams and pills, but the bumps did not get much better.\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Diagnosis: Molluscum contagiosum (a common skin infection that causes small bumps)\n",
      "\n",
      "Hospital Course Summary:\n",
      "The patient was given a pill called isotretinoin at a dose based on body weight for one month. This medicine was chosen because other treatments did not work and the bumps were spread across the body. Isotretinoin works inside the body to slow down skin cell growth. After one month, the bumps looked and felt better. The patient had no bad reactions to the medicine except dry lips. The dry lips were treated with petroleum jelly. The bumps stayed gone even after two months of checkups.\n",
      "\n",
      "Condition at Discharge:\n",
      "The patient left the clinic with much better skin. Almost no bumps were left on the right side of the face. A few bumps were still on the left side of the face. The patient was told to keep taking the medicine and come back for checkups.\n",
      "\n",
      "Instructions at Discharge:\n",
      "The patient was told to come back for more checkups. The patient was told to keep the skin clean and avoid sharing things like towels and clothes. This helps stop the infection from spreading. The patient was told to keep using petroleum jelly for dry lips. The patient was told to keep taking the isotretinoin pill as directed.\n",
      "\n",
      "Follow-Up Recommendations:\n",
      "The doctor told the patient to come back for regular checkups as required by Nepal health rules. The patient should take HIV medicine as directed and get regular checkups. The patient was told to call the doctor right away if any bad reactions or new problems happen.\n",
      "\n",
      "Signed,\n",
      "[REDACTED] (Clinician)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating simplified text with OpenBioLLM-8B...\")\n",
    "print(\"(This may take 1-2 minutes on CPU/MPS)\\n\")\n",
    "\n",
    "output_openbio = run_inference(model_openbio, tokenizer_openbio, medical_text, format_openbio)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"OpenBioLLM-8B OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(output_openbio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Free memory before loading next model\n",
    "del model_openbio, tokenizer_openbio\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference with Mistral-7B (Best Readability)\n",
    "\n",
    "**Best readability score:**\n",
    "- ROUGE-L: 0.6491\n",
    "- SARI: 73.79\n",
    "- BERTScore: 0.9464\n",
    "- FK-Grade: **6.91** (closest to target ‚â§6)\n",
    "- Improvement: +65.9% over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading Mistral-7B with LoRA adapter...\n",
      "======================================================================\n",
      "  Loading base model: mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656a34f0b39f4a14822ca6fc8a600f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading LoRA adapter: GuyDor007/MediSimplifier-LoRA-Adapters/mistral_7b_lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae6a55eb78342b681012d03a1daeb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1efbe831659745b899ad8e9f604262bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mistral_7b_lora/adapter_model.safetensor(‚Ä¶):   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading tokenizer...\n",
      "\n",
      "‚úÖ Mistral-7B loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading Mistral-7B with LoRA adapter...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_mistral, tokenizer_mistral, format_mistral = load_model(\"Mistral-7B\")\n",
    "\n",
    "print(\"\\n‚úÖ Mistral-7B loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simplified text with Mistral-7B...\n",
      "(This may take 1-2 minutes on CPU/MPS)\n",
      "\n",
      "======================================================================\n",
      "Mistral-7B OUTPUT\n",
      "======================================================================\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The patient came to our clinic with many skin-colored bumps on the face and neck. The bumps were round and raised. They slowly grew bigger and more bumps appeared over one year. The patient told us he has HIV. His immune cell count was 82 cells per cubic millimeter. The patient had been taking HIV medicine for five years. The doctors found out what was wrong because some bumps had a small white head inside. The patient was confirmed to have a skin infection called molluscum contagiosum. The patient had not tried skin creams or other medicines before. The patient was given skin creams and pills to try, but the bumps did not get better.\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Diagnosis: Molluscum contagiosum (a skin infection that causes small bumps)\n",
      "\n",
      "Hospital Course Summary:\n",
      "The patient was given a pill called isotretinoin at a dose based on body weight for one month. This was chosen because other treatments were not available and the infection was spread across the face. Isotretinoin works by changing into a form that kills infected cells and slows down skin growth. After one month, the bumps looked better and there were fewer bumps. The patient had no bad reactions to the pill except dry lips. Dry lips were treated with petroleum jelly. The bumps stayed gone even after two months of checkups.\n",
      "\n",
      "Condition at Discharge:\n",
      "The patient left the clinic with almost no bumps on the right side of the face. A few bumps were still on the left side of the face. The patient was told to keep using the prescribed treatment. The patient was also told to come back for checkups with the doctor.\n",
      "\n",
      "Instructions at Discharge:\n",
      "The patient was told to come back to see the doctor for more checkups. The patient was told to keep the area clean and not share personal items like towels or clothes. This helps stop the infection from spreading. The patient was told to keep using petroleum jelly for dry lips. The patient was told to keep taking the same dose of isotretinoin pills.\n",
      "\n",
      "Follow-Up Recommendations:\n",
      "The doctor told the patient to come back for regular checkups. This follows national health rules in Nepal. The doctor also wants to make sure the patient takes all HIV medicine as directed. The doctor will watch how well the patient is doing. The patient was told to call the doctor right away if any bad reactions happen or if any new symptoms appear.\n",
      "\n",
      "Signed,\n",
      "[REDACTED] (Clinician)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating simplified text with Mistral-7B...\")\n",
    "print(\"(This may take 1-2 minutes on CPU/MPS)\\n\")\n",
    "\n",
    "output_mistral = run_inference(model_mistral, tokenizer_mistral, medical_text, format_mistral)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Mistral-7B OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(output_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Free memory before loading next model\n",
    "del model_mistral, tokenizer_mistral\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference with BioMistral-7B (Medical Baseline)\n",
    "\n",
    "**Medical domain baseline:**\n",
    "- ROUGE-L: 0.6318\n",
    "- SARI: 73.01\n",
    "- BERTScore: 0.9439\n",
    "- FK-Grade: 6.95\n",
    "- Improvement: +53.3% over baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 2581d733-38aa-49e0-a679-c874437a9938)')' thrown while requesting HEAD https://huggingface.co/BioMistral/BioMistral-7B-DARE/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Loading BioMistral-7B with LoRA adapter...\n",
      "======================================================================\n",
      "  Loading base model: BioMistral/BioMistral-7B-DARE\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16bb6d8a262d4804b869829dc4feef8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading LoRA adapter: GuyDor007/MediSimplifier-LoRA-Adapters/biomistral_7b_dare_lora\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd7619088cd426ea7d7eeed2e11c7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522644c942c8433a9256da4fd4c63ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "biomistral_7b_dare_lora/adapter_model.sa(‚Ä¶):   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6ad1259d8345eda9fef80704eaec5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdbd1ca4fc34b03b8cd6744e36243f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d5c4cd4e854b12b193cfe341119680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7bfa2acd4940c3818a38ccb474398c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ BioMistral-7B loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Loading BioMistral-7B with LoRA adapter...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_biomistral, tokenizer_biomistral, format_biomistral = load_model(\"BioMistral-7B\")\n",
    "\n",
    "print(\"\\n‚úÖ BioMistral-7B loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating simplified text with BioMistral-7B...\n",
      "(This may take 1-2 minutes on CPU/MPS)\n",
      "\n",
      "======================================================================\n",
      "BioMistral-7B OUTPUT\n",
      "======================================================================\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The patient came to our clinic with many small, skin-colored bumps on the face and neck. These bumps slowly grew in size and number over one year. The patient told us he has HIV. His immune cell count was 82, which is lower than normal. The patient has been taking HIV medicines for the past five years. The doctors found out what was causing the bumps. They saw small, round, raised spots in some of the bumps. The patient was told he has a common skin infection called molluscum contagiosum. The patient had tried many skin creams and pills before, but they did not help. The doctors gave the patient several skin creams and pills to use. The bumps did not get much better.\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Diagnosis: Molluscum contagiosum (a common skin infection)\n",
      "\n",
      "Hospital Course Summary:\n",
      "The patient took a medicine called isotretinoin by mouth for one month. This medicine was chosen because other treatments did not work and the bumps were spread across the face. Isotretinoin was used because it works inside the body and affects how cells grow and change. After one month, the patient and the doctor saw that the bumps looked better and there were fewer of them. The patient had no bad side effects from the medicine. The only problem was dry lips. This was treated with a moisturizing cream called petrolatum jelly. The bumps stayed gone even two months after the treatment ended.\n",
      "\n",
      "Condition at Discharge:\n",
      "The patient left the clinic with fewer bumps on the right side of the face. A few bumps still remained on the left side of the face. The patient was told to keep taking the medicine and go to follow-up visits with the doctor.\n",
      "\n",
      "Instructions at Discharge:\n",
      "The patient was told to see the doctor again for more check-ups. The patient should keep the area clean and wash hands often. The patient should not share personal items like towels and clothes to stop the infection from spreading. The patient should keep using petrolatum jelly for dry lips. The patient should keep taking the isotretinoin pills as directed.\n",
      "\n",
      "Follow-Up Recommendations:\n",
      "The doctor told the patient to come back for regular check-ups. This follows the national guidelines for Nepal. The patient should take all HIV medicines as directed and keep track of them. The patient should call the doctor right away if any bad side effects or new symptoms happen.\n",
      "\n",
      "Signed,\n",
      "[REDACTED] (Clinician)\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating simplified text with BioMistral-7B...\")\n",
    "print(\"(This may take 1-2 minutes on CPU/MPS)\\n\")\n",
    "\n",
    "output_biomistral = run_inference(model_biomistral, tokenizer_biomistral, medical_text, format_biomistral)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BioMistral-7B OUTPUT\")\n",
    "print(\"=\" * 70)\n",
    "print(output_biomistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# Free memory\n",
    "del model_biomistral, tokenizer_biomistral\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GROUND TRUTH (Claude-generated reference)\n",
      "======================================================================\n",
      "Discharge Summary:\n",
      "\n",
      "Patient Name: [REDACTED]\n",
      "Medical Record Number: [REDACTED]\n",
      "\n",
      "Hospital Course:\n",
      "The patient came to our clinic with skin-colored, round, raised bumps on the face and neck. These bumps had been growing in size and number over one year. The patient shared that he has HIV, and his immune cell count was very low at 82 cells per cubic millimeter. The patient had been taking HIV medication for the last five years. The doctor diagnosed the condition by looking at the bumps, which had a typical appearance. The patient was confirmed to have molluscum contagiosum, a viral skin infection. The patient had not taken any pills for this skin problem before. The patient was given creams and pills, but the bumps did not get much better.\n",
      "\n",
      "Discharge Diagnosis:\n",
      "Diagnosis: Molluscum contagiosum, a viral skin infection\n",
      "\n",
      "Hospital Course Summary:\n",
      "The patient was given a pill called isotretinoin at a dose based on body weight for one month. This treatment was chosen because other proven treatments were not available and the bumps were spread over a large area. Isotretinoin was used because it helps control how skin cells grow and change. After one month, the bumps looked better and there were fewer of them. The patient felt the improvement too. The only side effect was dry lips, which were treated with petroleum jelly. The improvement lasted even after two months of check-ups.\n",
      "\n",
      "Condition at Discharge:\n",
      "The patient left the clinic with much better skin. The right side of the face had almost no bumps left. A few bumps remained on the left side of the face. The patient was told to keep taking the medicine and to come back for follow-up visits.\n",
      "\n",
      "Instructions at Discharge:\n",
      "The patient was told to come back to see the doctor for more check-ups. The patient was told to keep clean and not share towels or clothes with others. This helps stop the infection from spreading. The patient was told to keep using petroleum jelly for dry lips. The patient was told to keep taking the isotretinoin pills as directed.\n",
      "\n",
      "Follow-Up Recommendations:\n",
      "The doctor told the patient to come back for regular visits, following the health guidelines of Nepal. This will help make sure the patient takes HIV medication correctly and stays healthy. The patient was told to call the doctor right away if there are any side effects or new health problems.\n",
      "\n",
      "Signed,\n",
      "[REDACTED] (Clinician)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GROUND TRUTH (Claude-generated reference)\")\n",
    "print(\"=\" * 70)\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LENGTH COMPARISON\n",
      "======================================================================\n",
      "Source                      Characters      Ratio\n",
      "--------------------------------------------------\n",
      "Original Input                   2,609       1.00\n",
      "Ground Truth                     2,377       0.91\n",
      "OpenBioLLM-8B                    2,229       0.85\n",
      "Mistral-7B                       2,475       0.95\n",
      "BioMistral-7B                    2,460       0.94\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LENGTH COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Source':<25} {'Characters':>12} {'Ratio':>10}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Original Input':<25} {len(medical_text):>12,} {1.00:>10.2f}\")\n",
    "print(f\"{'Ground Truth':<25} {len(ground_truth):>12,} {len(ground_truth)/len(medical_text):>10.2f}\")\n",
    "print(f\"{'OpenBioLLM-8B':<25} {len(output_openbio):>12,} {len(output_openbio)/len(medical_text):>10.2f}\")\n",
    "print(f\"{'Mistral-7B':<25} {len(output_mistral):>12,} {len(output_mistral)/len(medical_text):>10.2f}\")\n",
    "print(f\"{'BioMistral-7B':<25} {len(output_biomistral):>12,} {len(output_biomistral)/len(medical_text):>10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Performance Summary\n",
    "\n",
    "Results from full test set evaluation (1,001 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FULL TEST SET PERFORMANCE (1,001 samples)\n",
      "================================================================================\n",
      "Model                   ROUGE-L       SARI    BERTScore   FK-Grade    Œî vs Base\n",
      "--------------------------------------------------------------------------------\n",
      "OpenBioLLM-8B üèÜ          0.6749      74.64       0.9498       7.16      +157.3%\n",
      "Mistral-7B               0.6491      73.79       0.9464       6.91       +65.9%\n",
      "BioMistral-7B            0.6318      73.01       0.9439       6.95       +53.3%\n",
      "--------------------------------------------------------------------------------\n",
      "Ground Truth FK:           7.23\n",
      "Source FK:                14.50\n",
      "Target FK:                 ‚â§6.0\n",
      "\n",
      "üìä All models achieve ~50% readability reduction (college ‚Üí 7th grade level)\n"
     ]
    }
   ],
   "source": [
    "# Performance metrics from full evaluation\n",
    "print(\"=\" * 80)\n",
    "print(\"FULL TEST SET PERFORMANCE (1,001 samples)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'ROUGE-L':>10} {'SARI':>10} {'BERTScore':>12} {'FK-Grade':>10} {'Œî vs Base':>12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'OpenBioLLM-8B üèÜ':<20} {'0.6749':>10} {'74.64':>10} {'0.9498':>12} {'7.16':>10} {'+157.3%':>12}\")\n",
    "print(f\"{'Mistral-7B':<20} {'0.6491':>10} {'73.79':>10} {'0.9464':>12} {'6.91':>10} {'+65.9%':>12}\")\n",
    "print(f\"{'BioMistral-7B':<20} {'0.6318':>10} {'73.01':>10} {'0.9439':>12} {'6.95':>10} {'+53.3%':>12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Ground Truth FK:':<20} {'7.23':>10}\")\n",
    "print(f\"{'Source FK:':<20} {'14.50':>10}\")\n",
    "print(f\"{'Target FK:':<20} {'‚â§6.0':>10}\")\n",
    "print(\"\\nüìä All models achieve ~50% readability reduction (college ‚Üí 7th grade level)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings\n",
    "\n",
    "### Ranking Reversal\n",
    "The worst zero-shot model (OpenBioLLM) achieved the **best** fine-tuned performance:\n",
    "\n",
    "| Model | Zero-Shot ROUGE-L | Fine-Tuned ROUGE-L | Improvement |\n",
    "|-------|-------------------|--------------------|--------------|\n",
    "| OpenBioLLM-8B | 0.2623 (worst) | **0.6749** (best) | +157% |\n",
    "| Mistral-7B | 0.3912 | 0.6491 | +66% |\n",
    "| BioMistral-7B | 0.4120 (best) | 0.6318 (worst) | +53% |\n",
    "\n",
    "### Statistical Significance\n",
    "- All pairwise ROUGE-L differences are significant (p < 0.001)\n",
    "- Effect size: OpenBioLLM vs BioMistral = medium (Cohen's d = 0.79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resources\n",
    "\n",
    "### HuggingFace\n",
    "- **Models:** https://huggingface.co/GuyDor007/MediSimplifier-LoRA-Adapters\n",
    "- **Dataset:** https://huggingface.co/datasets/GuyDor007/medisimplifier-dataset\n",
    "\n",
    "### GitHub\n",
    "- **Code & Notebooks:** https://github.com/gd007/MediSimplifier\n",
    "\n",
    "### Prompt Formats\n",
    "Models were trained with their **native formats**:\n",
    "\n",
    "**ChatML (OpenBioLLM-8B):**\n",
    "```\n",
    "<|im_start|>system\n",
    "{system_message}<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "\n",
    "{input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "**Mistral (Mistral-7B, BioMistral-7B):**\n",
    "```\n",
    "[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "\n",
    "{input} [/INST]\n",
    "```\n",
    "\n",
    "### Citation\n",
    "```bibtex\n",
    "@misc{medisimplifier2026,\n",
    "  title={MediSimplifier: Medical Discharge Summary Simplification using LoRA Fine-Tuning},\n",
    "  author={Dor, Guy and Avraham, Shmulik},\n",
    "  year={2026},\n",
    "  institution={Technion - Israel Institute of Technology}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "**End of Notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
